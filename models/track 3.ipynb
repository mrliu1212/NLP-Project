{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94412aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model\n",
      "Evaluation model\n",
      "Validation Macro F1 Score (retrained best model): 0.4261\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import ParameterGrid, cross_val_score\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv('../data/train.csv')\n",
    "dev_df = pd.read_csv('../data/dev.csv')\n",
    "test_df = pd.read_csv('../data/test.csv')\n",
    "\n",
    "# Preprocessing\n",
    "full_train_texts = train_df['text'].tolist() + dev_df['text'].tolist()\n",
    "full_train_labels = train_df['label'].tolist() + dev_df['label'].tolist()\n",
    "\n",
    "labels = list(sorted(set(full_train_labels)))\n",
    "label2id = {label: idx for idx, label in enumerate(labels)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "y_train = [label2id[label] for label in train_df['label']]\n",
    "y_dev = [label2id[label] for label in dev_df['label']]\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "tfidf = TfidfVectorizer(\n",
    "    analyzer='char_wb',\n",
    "    ngram_range=(2,6),\n",
    "    max_features=50000\n",
    ")\n",
    "\n",
    "tfidf.fit(full_train_texts)\n",
    "\n",
    "X_train = tfidf.transform(train_df['text'])\n",
    "X_dev = tfidf.transform(dev_df['text'])\n",
    "X_test = tfidf.transform(test_df['text'])\n",
    "\n",
    "\n",
    "# Too time consuming TODO: Try with cv before submitting\n",
    "# # Hyperparameter Grid\n",
    "# param_grid = {\n",
    "#     'C': [0.01, 10],\n",
    "#     'l1_ratio': [0.3, 0.7]\n",
    "# }\n",
    "# grid = list(ParameterGrid(param_grid))\n",
    "\n",
    "# # Cache file\n",
    "# CACHE_FILE = './cache/ENET_CV5_cache.pkl'\n",
    "\n",
    "# # Load previous cache if exists\n",
    "# if os.path.exists(CACHE_FILE):\n",
    "#     with open(CACHE_FILE, 'rb') as f:\n",
    "#         cache = pickle.load(f)\n",
    "#     print(f\"Loaded {len(cache)} cached results.\")\n",
    "# else:\n",
    "#     cache = {}\n",
    "\n",
    "# best_score = 0\n",
    "# best_params = None\n",
    "\n",
    "# # Define scoring\n",
    "# scorer = make_scorer(f1_score, average='macro')\n",
    "\n",
    "# # Search\n",
    "# for params in tqdm(grid, desc=\"Grid Search with 5-Fold CV\"):\n",
    "#     param_key = tuple(sorted(params.items()))  # make a hashable key\n",
    "\n",
    "#     if param_key in cache:\n",
    "#         macro_f1 = cache[param_key]\n",
    "#     else:\n",
    "#         print(f'Start of parameter tuning: {param_key}')\n",
    "#         model = LogisticRegression(\n",
    "#             penalty='elasticnet',\n",
    "#             solver='saga',\n",
    "#             max_iter=1000,\n",
    "#             random_state=42,\n",
    "#             n_jobs=-1,\n",
    "#             **params\n",
    "#         )\n",
    "#         # Perform 5-Fold CV here!\n",
    "#         scores = cross_val_score(model, X_train, y_train, cv=5, scoring=scorer, n_jobs=-1)\n",
    "#         macro_f1 = np.mean(scores)\n",
    "\n",
    "#         cache[param_key] = macro_f1\n",
    "\n",
    "#         # Save cache every time\n",
    "#         with open(CACHE_FILE, 'wb') as f:\n",
    "#             pickle.dump(cache, f)\n",
    "#         print(f'Cached {param_key}, f1: {macro_f1:.4f}')\n",
    "\n",
    "#     if macro_f1 > best_score:\n",
    "#         best_score = macro_f1\n",
    "#         best_params = params\n",
    "\n",
    "# print(f\"Best parameters: {best_params}\")\n",
    "# print(f\"Best Macro F1 Score (5-Fold CV): {best_score:.4f}\")\n",
    "\n",
    "# Train the best model fully\n",
    "best_model = LogisticRegression(\n",
    "    penalty='elasticnet',\n",
    "    solver='saga',\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    C=1,\n",
    "    l1_ratio= 0.3\n",
    "    # **best_params\n",
    ")\n",
    "print(\"Fitting model\")\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate again on Dev Set\n",
    "print(\"Evaluation model\")\n",
    "dev_preds = best_model.predict(X_dev)\n",
    "dev_macro_f1 = f1_score(y_dev, dev_preds, average='macro')\n",
    "print(f\"Validation Macro F1 Score (retrained best model): {dev_macro_f1:.4f}\")\n",
    "# C=1, L1_ratio=0.7 -> 0.4172\n",
    "# C=1, L1_ratio=0.5 -> 0.4222\n",
    "# C=0.5, L1_ratio=0.5 -> 0.4024\n",
    "# C=1, L1_ratio=0.3 -> 0.4261\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a555716c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW  # <-- Correct import here!\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Load Data\n",
    "train_df = pd.read_csv('../data/train.csv')  # Replace with your path\n",
    "dev_df = pd.read_csv('../data/dev.csv')\n",
    "\n",
    "# PyTorch-ready dataset\n",
    "class RedditDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts              # Store the list of comment texts\n",
    "        self.labels = labels            # Store the corresponding list of integer labels\n",
    "        self.tokenizer = tokenizer      # Store the tokenizer (e.g., RobertaTokenizer)\n",
    "        self.max_length = max_length    # Store the maximum sequence length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)  # Important: this allows DataLoader to know dataset size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Fetch the text and its corresponding label using the index\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Tokenize the text:\n",
    "        # - truncation: cuts off texts longer than max_length\n",
    "        # - padding: adds padding tokens to shorter texts\n",
    "        # - return_tensors='pt': returns PyTorch tensors instead of lists\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length = self.max_length,\n",
    "            return_tensors='pt' # stands for PyTorch\n",
    "        )\n",
    "\n",
    "        # Return a dictionary with model inputs and label\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),         # Tensor of input token IDs .squeeze() again just removes the extra dimension.\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(), # Tensor indicating real tokens vs padding\n",
    "            'labels': torch.tensor(label, dtype=torch.long)        # Make sure it becomes a PyTorch Tensor, because the model expects labels to be Tensors during training.\n",
    "        }\n",
    "\n",
    "\n",
    "### FUNCTIONS: train, train_MPT, evaluate\n",
    "from torch.amp import GradScaler, autocast\n",
    "# Use smaller (half-size) numbers for most calculations, so training is much faster and uses less memory â€” without losing much accuracy.\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Mixed Precision Training\n",
    "def train_MPT(model, loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        with autocast(device_type=device.type):  # ðŸ”¥ autocast for mixed precision\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# Evaluation Function\n",
    "def evaluate(model, loader):\n",
    "    model.eval() # Set the model to evaluation mode\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad(): # Turns off gradient calculations (Saves memory and speeds up evaluation because we don't need gradients)\n",
    "        for batch in tqdm(loader):\n",
    "            # Move data to device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            # Get predictions (pick the class with the highest score (probability) for each example in the batch)\n",
    "            preds = torch.argmax(outputs.logits, dim=1) # outputs.logits = raw scores for each class\n",
    "            # Save predictions and true labels\n",
    "            # We Move predictions and labels from GPU to CPU and convert them into numpy arrays\n",
    "            # because some Python operations (like NumPy or scikit-learn) cannot work directly with GPU Tensors.\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Take the F1 score for each label (class) separately, and then average them equally (It does not care if one label has 1000 examples and another class has 100 examples)\n",
    "    macro_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    return macro_f1\n",
    "\n",
    "# -------------------------\n",
    "# Data preprocessing\n",
    "# -------------------------\n",
    "# Encode labels to integers\n",
    "labels = train_df['label'].unique()\n",
    "label2id = {label: idx for idx, label in enumerate(labels)} # {\"label\": 'id'}\n",
    "id2label = {idx: label for label, idx in label2id.items()} # {'id':'label}\n",
    "\n",
    "\n",
    "train_df['label_id'] = train_df['label'].map(label2id)\n",
    "dev_df['label_id'] = dev_df['label'].map(label2id)\n",
    "\n",
    "# -------------------\n",
    "# Prepare DataLoaders\n",
    "# -------------------\n",
    "# CONFIG\n",
    "\n",
    "# Version 1\n",
    "# MODEL_NAME = 'roberta-base'\n",
    "# MAX_LENGTH = 128\n",
    "# BATCH_SIZE = 64\n",
    "# EPOCHS = 2\n",
    "# LEARNING_RATE = 2e-5\n",
    "# 1)Train Loss: 1.4018, Validation Macro F1: 0.4303 2)Train Loss: 1.2615, Validation Macro F1: 0.4485\n",
    "\n",
    "# Version 2\n",
    "MODEL_NAME = 'roberta-base'\n",
    "MAX_LENGTH = 256\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 4\n",
    "LEARNING_RATE = 2e-5\n",
    "# 1)Train Loss: 1.3989, Validation Macro F1: 0.4421 2)Train Loss: 1.2495, Validation Macro F1: 0.4591\n",
    "# 3)Train Loss: 1.1111, Validation Macro F1: 0.4551 4)Train Loss: 0.9832, Validation Macro F1: 0.4615\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Prepare dataset for PyTorch (processes the raw data)\n",
    "train_dataset = RedditDataset(train_df['text'].tolist(), train_df['label_id'].tolist(), tokenizer, MAX_LENGTH)\n",
    "dev_dataset = RedditDataset(dev_df['text'].tolist(), dev_df['label_id'].tolist(), tokenizer, MAX_LENGTH)\n",
    "\n",
    "# Create small batches (batches them without changing the data itself)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # cuda is faster but it requires GPU\n",
    "model = RobertaForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(labels))\n",
    "model.to(device) # Moves all model weights and computations to the device you selected (cuda if GPU, otherwise cpu)\n",
    "\n",
    "# Optimizer and Scheduler\n",
    "# model.parameters() are all the learnable weights in the model.\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE) # A type of optimizer that updates the model's weights to minimize the loss. \n",
    "total_steps = len(train_loader) * EPOCHS \n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps) # It controls how the learning rate changes during training. In our case it linearly decreases \n",
    "\n",
    "\n",
    "MODEL_SAVE_PATH = f'cache/best_{MODEL_NAME}_model_{MAX_LENGTH}-{BATCH_SIZE}-{EPOCHS}-{LEARNING_RATE}.pt'\n",
    "\n",
    "# Check if saved model exists\n",
    "if os.path.exists(MODEL_SAVE_PATH):\n",
    "    print(f\"Loading saved model from {MODEL_SAVE_PATH}...\")\n",
    "    model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
    "else:\n",
    "    print(\"No saved model found. Starting training...\")\n",
    "\n",
    "    # Run Training\n",
    "    best_f1 = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(EPOCHS): # One epoch = One full pass through the entire training dataset.\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}\") \n",
    "        train_loss = train_MPT(model, train_loader)\n",
    "        val_f1 = evaluate(model, dev_loader)\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Validation Macro F1: {val_f1:.4f}\")\n",
    "\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            best_model_state = model.state_dict()\n",
    "\n",
    "    print(f'Best f1 for this model: {best_f1}')\n",
    "\n",
    "    # After training, save the best model\n",
    "    torch.save(best_model_state, MODEL_SAVE_PATH)\n",
    "    print(f\"Training finished. Best model saved to {MODEL_SAVE_PATH}\")\n",
    "\n",
    "    # Loads back the best version of the model that you saved during training (Forget your current weights â€” load these saved weights instead)\n",
    "    model.load_state_dict(best_model_state) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767fc49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Prepare Inputs for Meta Model\n",
    "# Elastic Net (TF-IDF) model outputs: dev_preds_enet_proba\n",
    "# RoBERTa model outputs: dev_logits_roberta\n",
    "\n",
    "# Get probabilities from Elastic Net (already using predict_proba)\n",
    "dev_preds_enet_proba = best_model.predict_proba(X_dev)  # shape: (n_samples, n_classes)\n",
    "\n",
    "# Get logits from RoBERTa\n",
    "roberta_model.eval()\n",
    "all_logits = []\n",
    "with torch.no_grad():\n",
    "    for batch in dev_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits.cpu().numpy()\n",
    "        all_logits.append(logits)\n",
    "\n",
    "dev_logits_roberta = np.vstack(all_logits)  # shape: (n_samples, n_classes)\n",
    "\n",
    "# Stack Features for Meta Model\n",
    "X_dev_stack = np.hstack([dev_preds_enet_proba, dev_logits_roberta])\n",
    "y_dev_array = np.array(y_dev)  # true labels for dev set\n",
    "\n",
    "# Train Meta Model\n",
    "meta_model = LogisticRegressionCV(cv=5, max_iter=1000, multi_class='multinomial')\n",
    "meta_model.fit(X_dev_stack, y_dev_array)\n",
    "\n",
    "# Predict and Evaluate\n",
    "meta_preds = meta_model.predict(X_dev_stack)\n",
    "stacked_f1 = f1_score(y_dev_array, meta_preds, average='macro')\n",
    "print(f\"Meta-model Validation Macro F1 Score: {stacked_f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
