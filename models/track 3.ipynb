{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git remote add origin \"https://github.com/mrliu1212/NLP-Project\""
      ],
      "metadata": {
        "id": "Rz9JWMjJ8n38"
      },
      "id": "Rz9JWMjJ8n38",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git fetch origin"
      ],
      "metadata": {
        "id": "DooOzHIO8zRP",
        "outputId": "81761da4-7fa7-4e63-ad84-da8e54094ce5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "DooOzHIO8zRP",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "remote: Enumerating objects: 141, done.\u001b[K\n",
            "remote: Counting objects: 100% (141/141), done.\u001b[K\n",
            "remote: Compressing objects: 100% (106/106), done.\u001b[K\n",
            "remote: Total 141 (delta 50), reused 112 (delta 31), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (141/141), 23.98 MiB | 8.00 MiB/s, done.\n",
            "Resolving deltas: 100% (50/50), done.\n",
            "From https://github.com/mrliu1212/NLP-Project\n",
            " * [new branch]      classification -> origin/classification\n",
            " * [new branch]      main           -> origin/main\n",
            " * [new branch]      media-bias     -> origin/media-bias\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git checkout classification"
      ],
      "metadata": {
        "id": "6dpCubSh83cb",
        "outputId": "426b5935-33c9-4f2d-a54a-90ce0826c9f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "6dpCubSh83cb",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtering content: 100% (2/2), 951.17 MiB | 45.97 MiB/s, done.\n",
            "Branch 'classification' set up to track remote branch 'classification' from 'origin'.\n",
            "Switched to a new branch 'classification'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git pull"
      ],
      "metadata": {
        "id": "y1pLUXbU87Od",
        "outputId": "177e63b8-c3e1-4f7d-a7fa-c4fa5e5d147b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "y1pLUXbU87Od",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already up to date.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd models"
      ],
      "metadata": {
        "id": "Wy8bEYqW9CGw",
        "outputId": "20657141-e34f-4c9a-93e2-e8c14182b48b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Wy8bEYqW9CGw",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/models\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "94412aaa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94412aaa",
        "outputId": "5c7e00e2-b781-4212-f060-ac6f234f77ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting model...\n",
            "Evaluation model\n",
            "Validation Macro F1 Score (retrained best model): 0.4261\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import ParameterGrid, cross_val_score\n",
        "from sklearn.metrics import make_scorer, f1_score\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "# Load data\n",
        "train_df = pd.read_csv('../data/train.csv')\n",
        "dev_df = pd.read_csv('../data/dev.csv')\n",
        "test_df = pd.read_csv('../data/test.csv')\n",
        "\n",
        "# Preprocessing\n",
        "full_train_texts = train_df['text'].tolist() + dev_df['text'].tolist()\n",
        "full_train_labels = train_df['label'].tolist() + dev_df['label'].tolist()\n",
        "\n",
        "labels = list(sorted(set(full_train_labels)))\n",
        "label2id = {label: idx for idx, label in enumerate(labels)}\n",
        "id2label = {idx: label for label, idx in label2id.items()}\n",
        "\n",
        "y_train = [label2id[label] for label in train_df['label']]\n",
        "y_dev = [label2id[label] for label in dev_df['label']]\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "tfidf = TfidfVectorizer(\n",
        "    analyzer='char_wb',\n",
        "    ngram_range=(2,6),\n",
        "    max_features=50000\n",
        ")\n",
        "\n",
        "tfidf.fit(full_train_texts)\n",
        "\n",
        "X_train = tfidf.transform(train_df['text'])\n",
        "X_dev = tfidf.transform(dev_df['text'])\n",
        "X_test = tfidf.transform(test_df['text'])\n",
        "\n",
        "# Train the best model fully\n",
        "best_model = LogisticRegression(\n",
        "    penalty='elasticnet',\n",
        "    solver='saga',\n",
        "    max_iter=1000,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    C=1,\n",
        "    l1_ratio= 0.3\n",
        "    # **best_params\n",
        ")\n",
        "print(\"Fitting model...\")\n",
        "best_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate again on Dev Set\n",
        "print(\"Evaluation model\")\n",
        "dev_preds = best_model.predict(X_dev)\n",
        "dev_macro_f1 = f1_score(y_dev, dev_preds, average='macro')\n",
        "print(f\"Validation Macro F1 Score (retrained best model): {dev_macro_f1:.4f}\")\n",
        "# C=1, L1_ratio=0.7 -> 0.4172\n",
        "# C=1, L1_ratio=0.5 -> 0.4222\n",
        "# C=0.5, L1_ratio=0.5 -> 0.4024\n",
        "# C=1, L1_ratio=0.3 -> 0.4261\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "import xgboost as xgb\n",
        "import joblib\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "SAVE_DIR = \"saved_models\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "# ========== Utility: Check if all models exist ==========\n",
        "def models_exist():\n",
        "    required_files = [\n",
        "        \"tfidf_vectorizer.pkl\",\n",
        "        \"logreg_model.pkl\",\n",
        "        \"xgb_model.pkl\",\n",
        "        \"meta_model.pkl\",\n",
        "        \"label_maps.pkl\"\n",
        "    ]\n",
        "    return all(os.path.exists(os.path.join(SAVE_DIR, fname)) for fname in required_files)\n",
        "\n",
        "# ========== Load Data ==========\n",
        "train_df = pd.read_csv('../data/train.csv')\n",
        "dev_df = pd.read_csv('../data/dev.csv')\n",
        "\n",
        "# ========== Train + Save Models If Not Exist ==========\n",
        "if not models_exist():\n",
        "    print(\"Models not found. Training models...\")\n",
        "\n",
        "    # Label Encoding\n",
        "    all_labels = sorted(set(train_df['label']) | set(dev_df['label']))\n",
        "    label2id = {label: i for i, label in enumerate(all_labels)}\n",
        "    id2label = {i: label for label, i in label2id.items()}\n",
        "    y_train = train_df['label'].map(label2id).values\n",
        "    y_dev = dev_df['label'].map(label2id).values\n",
        "\n",
        "    # TF-IDF Vectorization\n",
        "    print(\"Fitting TF-IDF vectorizer...\")\n",
        "    tfidf = TfidfVectorizer(analyzer='char_wb', ngram_range=(2, 6), max_features=50000)\n",
        "    tfidf.fit(pd.concat([train_df['text'], dev_df['text']]))\n",
        "\n",
        "    X_train = tfidf.transform(train_df['text'])\n",
        "    X_dev = tfidf.transform(dev_df['text'])\n",
        "\n",
        "    # Train base models\n",
        "    print(\"Training Logistic Regression...\")\n",
        "    logreg = LogisticRegression(\n",
        "        penalty='elasticnet',\n",
        "        solver='saga',\n",
        "        max_iter=1000,\n",
        "        C=1.0,\n",
        "        l1_ratio=0.3,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    logreg.fit(X_train, y_train)\n",
        "\n",
        "    print(\"Training XGBoost...\")\n",
        "    xgb_clf = xgb.XGBClassifier(\n",
        "        objective='multi:softprob',\n",
        "        num_class=len(label2id),\n",
        "        n_estimators=300,\n",
        "        learning_rate=0.1,\n",
        "        max_depth=4,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    xgb_clf.fit(X_train, y_train)\n",
        "\n",
        "    # Generate meta-features (dev set)\n",
        "    print(\"Generating meta-features...\")\n",
        "    dev_preds_logreg = logreg.predict_proba(X_dev)\n",
        "    dev_preds_xgb = xgb_clf.predict_proba(X_dev)\n",
        "    X_meta_dev = np.hstack([dev_preds_logreg, dev_preds_xgb])\n",
        "\n",
        "    # Train meta-model\n",
        "    print(\"Training meta-model...\")\n",
        "    meta_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "    meta_model.fit(X_meta_dev, y_dev)\n",
        "\n",
        "    # Evaluate\n",
        "    meta_dev_preds = meta_model.predict(X_meta_dev)\n",
        "    meta_f1 = f1_score(y_dev, meta_dev_preds, average='macro')\n",
        "    print(f\"Stacked Dev Macro F1: {meta_f1:.4f}\")\n",
        "\n",
        "    # Save models\n",
        "    print(\"Saving models...\")\n",
        "    with open(os.path.join(SAVE_DIR, \"tfidf_vectorizer.pkl\"), \"wb\") as f:\n",
        "        pickle.dump(tfidf, f)\n",
        "    joblib.dump(logreg, os.path.join(SAVE_DIR, \"logreg_model.pkl\"))\n",
        "    joblib.dump(xgb_clf, os.path.join(SAVE_DIR, \"xgb_model.pkl\"))\n",
        "    joblib.dump(meta_model, os.path.join(SAVE_DIR, \"meta_model.pkl\"))\n",
        "    with open(os.path.join(SAVE_DIR, \"label_maps.pkl\"), \"wb\") as f:\n",
        "        pickle.dump({'label2id': label2id, 'id2label': id2label}, f)\n",
        "\n",
        "    print(\"Models saved.\") # used 38 mins\n",
        "else:\n",
        "    print(\"Saved models already exist. Skipping training.\")\n",
        "     # Load models\n",
        "    print(\"Loading saved models...\")\n",
        "    with open(os.path.join(SAVE_DIR, \"tfidf_vectorizer.pkl\"), \"rb\") as f:\n",
        "        tfidf = pickle.load(f)\n",
        "    logreg = joblib.load(os.path.join(SAVE_DIR, \"logreg_model.pkl\"))\n",
        "    xgb_clf = joblib.load(os.path.join(SAVE_DIR, \"xgb_model.pkl\"))\n",
        "    meta_model = joblib.load(os.path.join(SAVE_DIR, \"meta_model.pkl\"))\n",
        "    with open(os.path.join(SAVE_DIR, \"label_maps.pkl\"), \"rb\") as f:\n",
        "        label_maps = pickle.load(f)\n",
        "        label2id = label_maps[\"label2id\"]\n",
        "        id2label = label_maps[\"id2label\"]\n",
        "\n",
        "    # Preprocess dev data\n",
        "    print(\"Vectorizing dev set...\")\n",
        "    X_dev = tfidf.transform(dev_df[\"text\"])\n",
        "    y_dev = dev_df[\"label\"].map(label2id).values\n",
        "\n",
        "    # Get base model predictions\n",
        "    print(\"Generating base model predictions...\")\n",
        "    dev_preds_logreg = logreg.predict_proba(X_dev)\n",
        "    dev_preds_xgb = xgb_clf.predict_proba(X_dev)\n",
        "\n",
        "    # Stack features\n",
        "    X_meta_dev = np.hstack([dev_preds_logreg, dev_preds_xgb])\n",
        "\n",
        "    # Predict with meta-model\n",
        "    print(\"Evaluating stacked model...\")\n",
        "    meta_dev_preds = meta_model.predict(X_meta_dev)\n",
        "    dev_preds_meta_proba = meta_model.predict_proba(X_meta_dev)\n",
        "    meta_f1 = f1_score(y_dev, meta_dev_preds, average=\"macro\")\n",
        "    print(f\"Stacked Dev Macro F1 Score (Loaded Models): {meta_f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "f3snkSIIOIXB",
        "outputId": "bfa58406-cdf6-459e-a468-61856f17027e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "f3snkSIIOIXB",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved models already exist. Skipping training.\n",
            "Loading saved models...\n",
            "Vectorizing dev set...\n",
            "Generating base model predictions...\n",
            "Evaluating stacked model...\n",
            "Stacked Dev Macro F1 Score (Loaded Models): 0.4316\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "a555716c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a555716c",
        "outputId": "a4b5f02e-d0e5-426e-c4a1-b138302495c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading saved model from cache/best_roberta-base_model_256-32-4-2e-05.pt...\n",
            "\n",
            "Evaluating loaded model on the dev set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 125/125 [00:51<00:00,  2.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Final Dev Macro F1 Score: 0.4615\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW  # <-- Correct import here!\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification, get_linear_schedule_with_warmup\n",
        "from sklearn.metrics import f1_score\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Load Data\n",
        "train_df = pd.read_csv('../data/train.csv')  # Replace with your path\n",
        "dev_df = pd.read_csv('../data/dev.csv')\n",
        "\n",
        "# PyTorch-ready dataset\n",
        "class RedditDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length):\n",
        "        self.texts = texts              # Store the list of comment texts\n",
        "        self.labels = labels            # Store the corresponding list of integer labels\n",
        "        self.tokenizer = tokenizer      # Store the tokenizer (e.g., RobertaTokenizer)\n",
        "        self.max_length = max_length    # Store the maximum sequence length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)  # Important: this allows DataLoader to know dataset size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Fetch the text and its corresponding label using the index\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Tokenize the text:\n",
        "        # - truncation: cuts off texts longer than max_length\n",
        "        # - padding: adds padding tokens to shorter texts\n",
        "        # - return_tensors='pt': returns PyTorch tensors instead of lists\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length = self.max_length,\n",
        "            return_tensors='pt' # stands for PyTorch\n",
        "        )\n",
        "\n",
        "        # Return a dictionary with model inputs and label\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(),         # Tensor of input token IDs .squeeze() again just removes the extra dimension.\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(), # Tensor indicating real tokens vs padding\n",
        "            'labels': torch.tensor(label, dtype=torch.long)        # Make sure it becomes a PyTorch Tensor, because the model expects labels to be Tensors during training.\n",
        "        }\n",
        "\n",
        "\n",
        "### FUNCTIONS: train, train_MPT, evaluate\n",
        "from torch.amp import GradScaler, autocast\n",
        "# Use smaller (half-size) numbers for most calculations, so training is much faster and uses less memory — without losing much accuracy.\n",
        "scaler = GradScaler()\n",
        "\n",
        "# Mixed Precision Training\n",
        "def train_MPT(model, loader):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        with autocast(device_type=device.type):  # 🔥 autocast for mixed precision\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "# Evaluation Function\n",
        "def evaluate(model, loader):\n",
        "    model.eval() # Set the model to evaluation mode\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad(): # Turns off gradient calculations (Saves memory and speeds up evaluation because we don't need gradients)\n",
        "        for batch in tqdm(loader):\n",
        "            # Move data to device\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            # Forward pass\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            # Get predictions (pick the class with the highest score (probability) for each example in the batch)\n",
        "            preds = torch.argmax(outputs.logits, dim=1) # outputs.logits = raw scores for each class\n",
        "            # Save predictions and true labels\n",
        "            # We Move predictions and labels from GPU to CPU and convert them into numpy arrays\n",
        "            # because some Python operations (like NumPy or scikit-learn) cannot work directly with GPU Tensors.\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Take the F1 score for each label (class) separately, and then average them equally (It does not care if one label has 1000 examples and another class has 100 examples)\n",
        "    macro_f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    return macro_f1\n",
        "\n",
        "# -------------------------\n",
        "# Data preprocessing\n",
        "# -------------------------\n",
        "# Encode labels to integers\n",
        "labels = train_df['label'].unique()\n",
        "label2id = {label: idx for idx, label in enumerate(labels)} # {\"label\": 'id'}\n",
        "id2label = {idx: label for label, idx in label2id.items()} # {'id':'label}\n",
        "\n",
        "\n",
        "train_df['label_id'] = train_df['label'].map(label2id)\n",
        "dev_df['label_id'] = dev_df['label'].map(label2id)\n",
        "\n",
        "# -------------------\n",
        "# Prepare DataLoaders\n",
        "# -------------------\n",
        "# CONFIG\n",
        "\n",
        "# Version 1\n",
        "# MODEL_NAME = 'roberta-base'\n",
        "# MAX_LENGTH = 128\n",
        "# BATCH_SIZE = 64\n",
        "# EPOCHS = 2\n",
        "# LEARNING_RATE = 2e-5\n",
        "# 1)Train Loss: 1.4018, Validation Macro F1: 0.4303 2)Train Loss: 1.2615, Validation Macro F1: 0.4485\n",
        "\n",
        "# Version 2\n",
        "MODEL_NAME = 'roberta-base'\n",
        "MAX_LENGTH = 256\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 4\n",
        "LEARNING_RATE = 2e-5\n",
        "# 1)Train Loss: 1.3989, Validation Macro F1: 0.4421 2)Train Loss: 1.2495, Validation Macro F1: 0.4591\n",
        "# 3)Train Loss: 1.1111, Validation Macro F1: 0.4551 4)Train Loss: 0.9832, Validation Macro F1: 0.4615\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Prepare dataset for PyTorch (processes the raw data)\n",
        "train_dataset = RedditDataset(train_df['text'].tolist(), train_df['label_id'].tolist(), tokenizer, MAX_LENGTH)\n",
        "dev_dataset = RedditDataset(dev_df['text'].tolist(), dev_df['label_id'].tolist(), tokenizer, MAX_LENGTH)\n",
        "\n",
        "# Create small batches (batches them without changing the data itself)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "# Model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # cuda is faster but it requires GPU\n",
        "model = RobertaForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(labels))\n",
        "model.to(device) # Moves all model weights and computations to the device you selected (cuda if GPU, otherwise cpu)\n",
        "\n",
        "# Optimizer and Scheduler\n",
        "# model.parameters() are all the learnable weights in the model.\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE) # A type of optimizer that updates the model's weights to minimize the loss.\n",
        "total_steps = len(train_loader) * EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps) # It controls how the learning rate changes during training. In our case it linearly decreases\n",
        "\n",
        "\n",
        "MODEL_SAVE_PATH = f'cache/best_{MODEL_NAME}_model_{MAX_LENGTH}-{BATCH_SIZE}-{EPOCHS}-{LEARNING_RATE}.pt'\n",
        "\n",
        "# Check if saved model exists\n",
        "if os.path.exists(MODEL_SAVE_PATH):\n",
        "    print(f\"Loading saved model from {MODEL_SAVE_PATH}...\")\n",
        "    model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
        "    print(\"\\nEvaluating loaded model on the dev set...\")\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dev_loader, desc=\"Evaluating\"):\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            preds = torch.argmax(outputs.logits, dim=1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    macro_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "    print(f\"\\n✅ Final Dev Macro F1 Score: {macro_f1:.4f}\")\n",
        "else:\n",
        "    print(\"No saved model found. Starting training...\")\n",
        "\n",
        "    # Run Training\n",
        "    best_f1 = 0\n",
        "    best_model_state = None\n",
        "\n",
        "    for epoch in range(EPOCHS): # One epoch = One full pass through the entire training dataset.\n",
        "        print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
        "        train_loss = train_MPT(model, train_loader)\n",
        "        val_f1 = evaluate(model, dev_loader)\n",
        "        print(f\"Train Loss: {train_loss:.4f}, Validation Macro F1: {val_f1:.4f}\")\n",
        "\n",
        "        if val_f1 > best_f1:\n",
        "            best_f1 = val_f1\n",
        "            best_model_state = model.state_dict()\n",
        "\n",
        "    print(f'Best f1 for this model: {best_f1}')\n",
        "\n",
        "    # After training, save the best model\n",
        "    torch.save(best_model_state, MODEL_SAVE_PATH)\n",
        "    print(f\"Training finished. Best model saved to {MODEL_SAVE_PATH}\")\n",
        "\n",
        "    # Loads back the best version of the model that you saved during training (Forget your current weights — load these saved weights instead)\n",
        "    model.load_state_dict(best_model_state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "767fc49b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "767fc49b",
        "outputId": "75f416fa-1549-4107-d775-cd3b99fda0a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1908: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meta-model Validation Macro F1 Score: 0.4655\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Prepare Inputs for Meta Model\n",
        "# Elastic Net (TF-IDF) model outputs: dev_preds_enet_proba\n",
        "# RoBERTa model outputs: dev_logits_roberta\n",
        "\n",
        "# Get probabilities from Elastic Net (already using predict_proba)\n",
        "dev_preds_enet_proba = best_model.predict_proba(X_dev)  # shape: (n_samples, n_classes)\n",
        "dev_preds_meta_proba = dev_preds_meta_proba\n",
        "\n",
        "# Get logits from RoBERTa\n",
        "model.eval()\n",
        "all_logits = []\n",
        "with torch.no_grad():\n",
        "    for batch in dev_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits.cpu().numpy()\n",
        "        all_logits.append(logits)\n",
        "\n",
        "dev_logits_roberta = np.vstack(all_logits)  # shape: (n_samples, n_classes)\n",
        "\n",
        "# Stack Features for Meta Model\n",
        "X_dev_stack = np.hstack([dev_preds_meta_proba, dev_logits_roberta])\n",
        "y_dev_array = np.array(y_dev)  # true labels for dev set\n",
        "\n",
        "# Train Meta Model\n",
        "meta_model = LogisticRegressionCV(cv=5, max_iter=1000, multi_class='multinomial')\n",
        "meta_model.fit(X_dev_stack, y_dev_array)\n",
        "\n",
        "# Predict and Evaluate\n",
        "meta_preds = meta_model.predict(X_dev_stack)\n",
        "stacked_f1 = f1_score(y_dev_array, meta_preds, average='macro')\n",
        "print(f\"Meta-model Validation Macro F1 Score: {stacked_f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2N1F4MXSKcm1",
        "outputId": "79e6ef99-a34e-4662-f0ad-698d77c84b2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "2N1F4MXSKcm1",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved models already exist. Skipping training.\n",
            "Loading saved models...\n",
            "Vectorizing dev set...\n",
            "Generating base model predictions...\n",
            "Evaluating stacked model...\n",
            "✅ Stacked Dev Macro F1 Score (Loaded Models): 0.4316\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "E0_8jtsFKcPJ"
      },
      "id": "E0_8jtsFKcPJ"
    },
    {
      "cell_type": "code",
      "source": [
        "# next"
      ],
      "metadata": {
        "id": "UKmNG-k-Kb2F"
      },
      "id": "UKmNG-k-Kb2F",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW  # <-- Correct import here!\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification, get_linear_schedule_with_warmup\n",
        "from sklearn.metrics import f1_score\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Load Data\n",
        "train_df = pd.read_csv('../data/train.csv')  # Replace with your path\n",
        "dev_df = pd.read_csv('../data/dev.csv')\n",
        "\n",
        "# PyTorch-ready dataset\n",
        "class RedditDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length):\n",
        "        self.texts = texts              # Store the list of comment texts\n",
        "        self.labels = labels            # Store the corresponding list of integer labels\n",
        "        self.tokenizer = tokenizer      # Store the tokenizer (e.g., RobertaTokenizer)\n",
        "        self.max_length = max_length    # Store the maximum sequence length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)  # Important: this allows DataLoader to know dataset size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Fetches a single sample from the dataset.\n",
        "\n",
        "        Args:\n",
        "            idx (int): Index of the item.\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary containing:\n",
        "                - 'input_ids': Raw text must be converted into numbers because models can't understand plain words.\n",
        "                - 'attention_mask': A list that tells the model which tokens are real (1) and which ones are just padding (0).\n",
        "                - 'labels': The label\n",
        "\n",
        "        * Tensor is a multi-dimensional array (like a super-powered NumPy array) that can run on a GPU very efficiently\n",
        "\n",
        "        ex:\n",
        "        Raw Text:\n",
        "        \"eating soap to own the republicans\"\n",
        "\n",
        "        Tokenized to IDs:\n",
        "        [0, 1553, 4153, 7, 1225, 5, 13815, 2, *1, *1]\n",
        "\n",
        "        Attention Mask:\n",
        "        [1, 1, 1, 1, 1, 1, 1, 1, *0, *0]\n",
        "\n",
        "        Label:\n",
        "        0\n",
        "\n",
        "        Models usually expect inputs to have the same length (max = max_lenght).\n",
        "        If your token list is too short, you add padding tokens (which are just zeros).\n",
        "        \"\"\"\n",
        "        # Fetch the text and its corresponding label using the index\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Tokenize the text:\n",
        "        # - truncation: cuts off texts longer than max_length\n",
        "        # - padding: adds padding tokens to shorter texts\n",
        "        # - return_tensors='pt': returns PyTorch tensors instead of lists\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length = self.max_length,\n",
        "            return_tensors='pt' # stands for PyTorch\n",
        "        )\n",
        "\n",
        "        # Return a dictionary with model inputs and label\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(),         # Tensor of input token IDs .squeeze() again just removes the extra dimension.\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(), # Tensor indicating real tokens vs padding\n",
        "            'labels': torch.tensor(label, dtype=torch.long)        # Make sure it becomes a PyTorch Tensor, because the model expects labels to be Tensors during training.\n",
        "        }\n",
        "\n",
        "        \"\"\"\n",
        "        squeeze example:\n",
        "\n",
        "        [\n",
        "        [ 0, 1553, 4153, 7, 1225, 5, 13815, 2, 1, 1 ]\n",
        "        ]\n",
        "        (batch size 1, 10 tokens)\n",
        "\n",
        "        to\n",
        "\n",
        "        [ 0, 1553, 4153, 7, 1225, 5, 13815, 2, 1, 1 ]\n",
        "        (just 10 tokens)\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "  ### FUNCTIONS: train, train_MPT, evaluate\n",
        "from torch.amp import GradScaler, autocast\n",
        "# Use smaller (half-size) numbers for most calculations, so training is much faster and uses less memory — without losing much accuracy.\n",
        "scaler = GradScaler()\n",
        "\n",
        "# Mixed Precision Training\n",
        "def train_MPT(model, loader):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        with autocast(device_type=device.type):  # 🔥 autocast for mixed precision\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "\n",
        "# Training Loop\n",
        "def train(model, loader):\n",
        "    model.train() # Sets the model into training mode. It should be done before training starts\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(loader):\n",
        "        optimizer.zero_grad() # Reset gradients: every time before a new update, clear the old gradients.\n",
        "\n",
        "        # Move data to device (Model and data must be on the same device to avoid errors)\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        # Forward pass (Feed the inputs into the model)\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Update model weights based on the gradients\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        # Track total loss (for reporting)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(loader) # average loss per batch\n",
        "\n",
        "\n",
        "# Evaluation Function\n",
        "def evaluate(model, loader):\n",
        "    model.eval() # Set the model to evaluation mode\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad(): # Turns off gradient calculations (Saves memory and speeds up evaluation because we don't need gradients)\n",
        "        for batch in tqdm(loader):\n",
        "            # Move data to device\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            # Forward pass\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            # Get predictions (pick the class with the highest score (probability) for each example in the batch)\n",
        "            preds = torch.argmax(outputs.logits, dim=1) # outputs.logits = raw scores for each class\n",
        "            # Save predictions and true labels\n",
        "            # We Move predictions and labels from GPU to CPU and convert them into numpy arrays\n",
        "            # because some Python operations (like NumPy or scikit-learn) cannot work directly with GPU Tensors.\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Take the F1 score for each label (class) separately, and then average them equally (It does not care if one label has 1000 examples and another class has 100 examples)\n",
        "    macro_f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    return macro_f1"
      ],
      "metadata": {
        "id": "ZfM0V9r89kid"
      },
      "id": "ZfM0V9r89kid",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "f3b829c9",
      "metadata": {
        "id": "f3b829c9",
        "outputId": "d0c0cf58-86f5-4cce-d12e-052514fa2933",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "RoBERTa Dev Inference: 100%|██████████| 125/125 [00:50<00:00,  2.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Stacked Dev Macro F1: 0.4579\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.metrics import f1_score\n",
        "import joblib\n",
        "import pickle\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "# -------------------------------\n",
        "# CONFIG\n",
        "# -------------------------------\n",
        "MODEL_NAME = 'roberta-base'\n",
        "MAX_LENGTH = 256\n",
        "BATCH_SIZE = 32\n",
        "MODEL_SAVE_PATH = f'cache/best_{MODEL_NAME}_model_{MAX_LENGTH}-{BATCH_SIZE}-4-2e-05.pt'\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# -------------------------------\n",
        "# Load Dev Data\n",
        "# -------------------------------\n",
        "dev_df = pd.read_csv('../data/dev.csv')\n",
        "\n",
        "with open(\"saved_models/label_maps.pkl\", \"rb\") as f:\n",
        "    label_maps = pickle.load(f)\n",
        "    label2id = label_maps[\"label2id\"]\n",
        "    id2label = label_maps[\"id2label\"]\n",
        "\n",
        "y_dev = dev_df[\"label\"].map(label2id).values\n",
        "\n",
        "# -------------------------------\n",
        "# Load TF-IDF and ElasticNet\n",
        "# -------------------------------\n",
        "with open(\"saved_models/tfidf_vectorizer.pkl\", \"rb\") as f:\n",
        "    tfidf = pickle.load(f)\n",
        "\n",
        "X_dev_tfidf = tfidf.transform(dev_df[\"text\"])\n",
        "enet_model = joblib.load(\"saved_models/logreg_model.pkl\")\n",
        "enet_dev_proba = enet_model.predict_proba(X_dev_tfidf)\n",
        "\n",
        "# -------------------------------\n",
        "# Load RoBERTa and Tokenizer\n",
        "# -------------------------------\n",
        "tokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "class RedditDataset(Dataset):\n",
        "    def __init__(self, texts, tokenizer, max_length):\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        encoding = self.tokenizer(\n",
        "            self.texts[idx],\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze()\n",
        "        }\n",
        "\n",
        "dev_dataset = RedditDataset(dev_df[\"text\"].tolist(), tokenizer, MAX_LENGTH)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "roberta_model = RobertaForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(label2id))\n",
        "roberta_model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
        "roberta_model.to(device)\n",
        "roberta_model.eval()\n",
        "\n",
        "# Get RoBERTa probabilities on dev set\n",
        "roberta_dev_proba = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(dev_loader, desc=\"RoBERTa Dev Inference\"):\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        outputs = roberta_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        probs = F.softmax(outputs.logits, dim=1)\n",
        "        roberta_dev_proba.append(probs.cpu().numpy())\n",
        "\n",
        "roberta_dev_proba = np.vstack(roberta_dev_proba)\n",
        "\n",
        "# -------------------------------\n",
        "# Stack Dev Features\n",
        "# -------------------------------\n",
        "X_meta_dev = np.hstack([enet_dev_proba, roberta_dev_proba])\n",
        "\n",
        "# Train meta-model\n",
        "meta_model = LogisticRegressionCV(cv=5, max_iter=1000)\n",
        "meta_model.fit(X_meta_dev, y_dev)\n",
        "\n",
        "# Predict + Evaluate\n",
        "meta_preds = meta_model.predict(X_meta_dev)\n",
        "meta_f1 = f1_score(y_dev, meta_preds, average='macro')\n",
        "print(f\"✅ Stacked Dev Macro F1: {meta_f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW  # <-- Correct import here!\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification, get_linear_schedule_with_warmup\n",
        "from sklearn.metrics import f1_score\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Load Data\n",
        "train_df = pd.read_csv('../data/train.csv')  # Replace with your path\n",
        "dev_df = pd.read_csv('../data/dev.csv')\n",
        "\n",
        "# PyTorch-ready dataset\n",
        "class RedditDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length):\n",
        "        self.texts = texts              # Store the list of comment texts\n",
        "        self.labels = labels            # Store the corresponding list of integer labels\n",
        "        self.tokenizer = tokenizer      # Store the tokenizer (e.g., RobertaTokenizer)\n",
        "        self.max_length = max_length    # Store the maximum sequence length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)  # Important: this allows DataLoader to know dataset size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Fetches a single sample from the dataset.\n",
        "\n",
        "        Args:\n",
        "            idx (int): Index of the item.\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary containing:\n",
        "                - 'input_ids': Raw text must be converted into numbers because models can't understand plain words.\n",
        "                - 'attention_mask': A list that tells the model which tokens are real (1) and which ones are just padding (0).\n",
        "                - 'labels': The label\n",
        "\n",
        "        * Tensor is a multi-dimensional array (like a super-powered NumPy array) that can run on a GPU very efficiently\n",
        "\n",
        "        ex:\n",
        "        Raw Text:\n",
        "        \"eating soap to own the republicans\"\n",
        "\n",
        "        Tokenized to IDs:\n",
        "        [0, 1553, 4153, 7, 1225, 5, 13815, 2, *1, *1]\n",
        "\n",
        "        Attention Mask:\n",
        "        [1, 1, 1, 1, 1, 1, 1, 1, *0, *0]\n",
        "\n",
        "        Label:\n",
        "        0\n",
        "\n",
        "        Models usually expect inputs to have the same length (max = max_lenght).\n",
        "        If your token list is too short, you add padding tokens (which are just zeros).\n",
        "        \"\"\"\n",
        "        # Fetch the text and its corresponding label using the index\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Tokenize the text:\n",
        "        # - truncation: cuts off texts longer than max_length\n",
        "        # - padding: adds padding tokens to shorter texts\n",
        "        # - return_tensors='pt': returns PyTorch tensors instead of lists\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length = self.max_length,\n",
        "            return_tensors='pt' # stands for PyTorch\n",
        "        )\n",
        "\n",
        "        # Return a dictionary with model inputs and label\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(),         # Tensor of input token IDs .squeeze() again just removes the extra dimension.\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(), # Tensor indicating real tokens vs padding\n",
        "            'labels': torch.tensor(label, dtype=torch.long)        # Make sure it becomes a PyTorch Tensor, because the model expects labels to be Tensors during training.\n",
        "        }\n",
        "\n",
        "        \"\"\"\n",
        "        squeeze example:\n",
        "\n",
        "        [\n",
        "        [ 0, 1553, 4153, 7, 1225, 5, 13815, 2, 1, 1 ]\n",
        "        ]\n",
        "        (batch size 1, 10 tokens)\n",
        "\n",
        "        to\n",
        "\n",
        "        [ 0, 1553, 4153, 7, 1225, 5, 13815, 2, 1, 1 ]\n",
        "        (just 10 tokens)\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### FUNCTIONS: train, train_MPT, evaluate\n",
        "from torch.amp import GradScaler, autocast\n",
        "# Use smaller (half-size) numbers for most calculations, so training is much faster and uses less memory — without losing much accuracy.\n",
        "scaler = GradScaler()\n",
        "\n",
        "# Mixed Precision Training\n",
        "def train_MPT(model, loader):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        with autocast(device_type=device.type):  # 🔥 autocast for mixed precision\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "\n",
        "# Training Loop\n",
        "def train(model, loader):\n",
        "    model.train() # Sets the model into training mode. It should be done before training starts\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(loader):\n",
        "        optimizer.zero_grad() # Reset gradients: every time before a new update, clear the old gradients.\n",
        "\n",
        "        # Move data to device (Model and data must be on the same device to avoid errors)\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        # Forward pass (Feed the inputs into the model)\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Update model weights based on the gradients\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        # Track total loss (for reporting)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(loader) # average loss per batch\n",
        "\n",
        "\n",
        "# Evaluation Function\n",
        "def evaluate(model, loader):\n",
        "    model.eval() # Set the model to evaluation mode\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad(): # Turns off gradient calculations (Saves memory and speeds up evaluation because we don't need gradients)\n",
        "        for batch in tqdm(loader):\n",
        "            # Move data to device\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            # Forward pass\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            # Get predictions (pick the class with the highest score (probability) for each example in the batch)\n",
        "            preds = torch.argmax(outputs.logits, dim=1) # outputs.logits = raw scores for each class\n",
        "            # Save predictions and true labels\n",
        "            # We Move predictions and labels from GPU to CPU and convert them into numpy arrays\n",
        "            # because some Python operations (like NumPy or scikit-learn) cannot work directly with GPU Tensors.\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Take the F1 score for each label (class) separately, and then average them equally (It does not care if one label has 1000 examples and another class has 100 examples)\n",
        "    macro_f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    return macro_f1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Data preprocessing\n",
        "# -------------------------\n",
        "# Encode labels to integers\n",
        "labels = train_df['label'].unique()\n",
        "label2id = {label: idx for idx, label in enumerate(labels)} # {\"label\": 'id'}\n",
        "id2label = {idx: label for label, idx in label2id.items()} # {'id':'label}\n",
        "\n",
        "\n",
        "train_df['label_id'] = train_df['label'].map(label2id)\n",
        "dev_df['label_id'] = dev_df['label'].map(label2id)\n",
        "\n",
        "# -------------------\n",
        "# Prepare DataLoaders\n",
        "# -------------------\n",
        "# CONFIG\n",
        "\n",
        "# Version 1\n",
        "# MODEL_NAME = 'roberta-base'\n",
        "# MAX_LENGTH = 128\n",
        "# BATCH_SIZE = 64\n",
        "# EPOCHS = 2\n",
        "# LEARNING_RATE = 2e-5\n",
        "# 1)Train Loss: 1.4018, Validation Macro F1: 0.4303 2)Train Loss: 1.2615, Validation Macro F1: 0.4485\n",
        "\n",
        "# Version 2\n",
        "MODEL_NAME = 'roberta-base'\n",
        "MAX_LENGTH = 256\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 4\n",
        "LEARNING_RATE = 2e-5\n",
        "# 1)Train Loss: 1.3989, Validation Macro F1: 0.4421 2)Train Loss: 1.2495, Validation Macro F1: 0.4591\n",
        "# 3)Train Loss: 1.1111, Validation Macro F1: 0.4551 4)Train Loss: 0.9832, Validation Macro F1: 0.4615\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Prepare dataset for PyTorch (processes the raw data)\n",
        "train_dataset = RedditDataset(train_df['text'].tolist(), train_df['label_id'].tolist(), tokenizer, MAX_LENGTH)\n",
        "dev_dataset = RedditDataset(dev_df['text'].tolist(), dev_df['label_id'].tolist(), tokenizer, MAX_LENGTH)\n",
        "\n",
        "# Create small batches (batches them without changing the data itself)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "# Model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # cuda is faster but it requires GPU\n",
        "model = RobertaForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(labels))\n",
        "model.to(device) # Moves all model weights and computations to the device you selected (cuda if GPU, otherwise cpu)\n",
        "\n",
        "# Optimizer and Scheduler\n",
        "# model.parameters() are all the learnable weights in the model.\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE) # A type of optimizer that updates the model's weights to minimize the loss.\n",
        "total_steps = len(train_loader) * EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps) # It controls how the learning rate changes during training. In our case it linearly decreases\n",
        "\n",
        "\n",
        "MODEL_SAVE_PATH = f'cache/best_{MODEL_NAME}_model_{MAX_LENGTH}-{BATCH_SIZE}-{EPOCHS}-{LEARNING_RATE}.pt'\n",
        "\n",
        "# Check if saved model exists\n",
        "if os.path.exists(MODEL_SAVE_PATH):\n",
        "    print(f\"Loading saved model from {MODEL_SAVE_PATH}...\")\n",
        "    model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
        "else:\n",
        "    print(\"No saved model found. Starting training...\")\n",
        "\n",
        "    # Run Training\n",
        "    best_f1 = 0\n",
        "    best_model_state = None\n",
        "\n",
        "    for epoch in range(EPOCHS): # One epoch = One full pass through the entire training dataset.\n",
        "        print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
        "        train_loss = train_MPT(model, train_loader)\n",
        "        val_f1 = evaluate(model, dev_loader)\n",
        "        print(f\"Train Loss: {train_loss:.4f}, Validation Macro F1: {val_f1:.4f}\")\n",
        "\n",
        "        if val_f1 > best_f1:\n",
        "            best_f1 = val_f1\n",
        "            best_model_state = model.state_dict()\n",
        "\n",
        "    print(f'Best f1 for this model: {best_f1}')\n",
        "\n",
        "    # After training, save the best model\n",
        "    torch.save(best_model_state, MODEL_SAVE_PATH)\n",
        "    print(f\"Training finished. Best model saved to {MODEL_SAVE_PATH}\")\n",
        "\n",
        "    # Loads back the best version of the model that you saved during training (Forget your current weights — load these saved weights instead)\n",
        "    model.load_state_dict(best_model_state)\n"
      ],
      "metadata": {
        "id": "0mQbGzXuFETv",
        "outputId": "70a4594a-98f1-4d19-d16c-2931f31fbe98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "0mQbGzXuFETv",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading saved model from cache/best_roberta-base_model_256-32-4-2e-05.pt...\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}