{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d40a55c3",
      "metadata": {},
      "source": [
        "# TFIDF + Elastic Net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da6802e8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting model\n",
            "Evaluation model\n",
            "Validation Macro F1 Score (retrained best model): 0.4261\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import ParameterGrid, cross_val_score\n",
        "from sklearn.metrics import make_scorer, f1_score\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "# Load data\n",
        "train_df = pd.read_csv('../data/train.csv')\n",
        "dev_df = pd.read_csv('../data/dev.csv')\n",
        "test_df = pd.read_csv('../data/test.csv')\n",
        "\n",
        "# Preprocessing\n",
        "full_train_texts = train_df['text'].tolist() + dev_df['text'].tolist()\n",
        "full_train_labels = train_df['label'].tolist() + dev_df['label'].tolist()\n",
        "\n",
        "labels = list(sorted(set(full_train_labels)))\n",
        "label2id = {label: idx for idx, label in enumerate(labels)}\n",
        "id2label = {idx: label for label, idx in label2id.items()}\n",
        "\n",
        "y_train = [label2id[label] for label in train_df['label']]\n",
        "y_dev = [label2id[label] for label in dev_df['label']]\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "tfidf = TfidfVectorizer(\n",
        "    analyzer='char_wb',\n",
        "    ngram_range=(2,6),\n",
        "    max_features=50000\n",
        ")\n",
        "\n",
        "tfidf.fit(full_train_texts)\n",
        "\n",
        "X_train = tfidf.transform(train_df['text'])\n",
        "X_dev = tfidf.transform(dev_df['text'])\n",
        "X_test = tfidf.transform(test_df['text'])\n",
        "\n",
        "# Train the best model fully\n",
        "best_model = LogisticRegression(\n",
        "    penalty='elasticnet',\n",
        "    solver='saga',\n",
        "    max_iter=1000,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    C=1,\n",
        "    l1_ratio= 0.3\n",
        "    # **best_params\n",
        ")\n",
        "print(\"Fitting model\")\n",
        "best_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate again on Dev Set\n",
        "print(\"Evaluation model\")\n",
        "dev_preds = best_model.predict(X_dev)\n",
        "dev_macro_f1 = f1_score(y_dev, dev_preds, average='macro')\n",
        "print(f\"Validation Macro F1 Score (retrained best model): {dev_macro_f1:.4f}\")\n",
        "# C=1, L1_ratio=0.7 -> 0.4172\n",
        "# C=1, L1_ratio=0.5 -> 0.4222\n",
        "# C=0.5, L1_ratio=0.5 -> 0.4024\n",
        "# C=1, L1_ratio=0.3 -> 0.4261\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ba4a31b",
      "metadata": {},
      "source": [
        "# TFIDF + XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8967530",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vectorizing data\n",
            "Vectorization complete.\n",
            "Training XGBoost...\n",
            "[0]\tvalidation_0-mlogloss:1.59660\tvalidation_1-mlogloss:1.59694\n",
            "[1]\tvalidation_0-mlogloss:1.58542\tvalidation_1-mlogloss:1.58588\n",
            "[2]\tvalidation_0-mlogloss:1.57677\tvalidation_1-mlogloss:1.57749\n",
            "[3]\tvalidation_0-mlogloss:1.56816\tvalidation_1-mlogloss:1.56938\n",
            "[4]\tvalidation_0-mlogloss:1.56004\tvalidation_1-mlogloss:1.56188\n",
            "[5]\tvalidation_0-mlogloss:1.55268\tvalidation_1-mlogloss:1.55508\n",
            "[6]\tvalidation_0-mlogloss:1.54618\tvalidation_1-mlogloss:1.54884\n",
            "[7]\tvalidation_0-mlogloss:1.54010\tvalidation_1-mlogloss:1.54338\n",
            "[8]\tvalidation_0-mlogloss:1.53430\tvalidation_1-mlogloss:1.53803\n",
            "[9]\tvalidation_0-mlogloss:1.52921\tvalidation_1-mlogloss:1.53344\n",
            "[10]\tvalidation_0-mlogloss:1.52471\tvalidation_1-mlogloss:1.52969\n",
            "[11]\tvalidation_0-mlogloss:1.52035\tvalidation_1-mlogloss:1.52629\n",
            "[12]\tvalidation_0-mlogloss:1.51612\tvalidation_1-mlogloss:1.52260\n",
            "[13]\tvalidation_0-mlogloss:1.51210\tvalidation_1-mlogloss:1.51908\n",
            "[14]\tvalidation_0-mlogloss:1.50845\tvalidation_1-mlogloss:1.51568\n",
            "[15]\tvalidation_0-mlogloss:1.50501\tvalidation_1-mlogloss:1.51259\n",
            "[16]\tvalidation_0-mlogloss:1.50178\tvalidation_1-mlogloss:1.51010\n",
            "[17]\tvalidation_0-mlogloss:1.49856\tvalidation_1-mlogloss:1.50730\n",
            "[18]\tvalidation_0-mlogloss:1.49551\tvalidation_1-mlogloss:1.50450\n",
            "[19]\tvalidation_0-mlogloss:1.49258\tvalidation_1-mlogloss:1.50206\n",
            "[20]\tvalidation_0-mlogloss:1.49006\tvalidation_1-mlogloss:1.49987\n",
            "[21]\tvalidation_0-mlogloss:1.48742\tvalidation_1-mlogloss:1.49764\n",
            "[22]\tvalidation_0-mlogloss:1.48489\tvalidation_1-mlogloss:1.49552\n",
            "[23]\tvalidation_0-mlogloss:1.48229\tvalidation_1-mlogloss:1.49398\n",
            "[24]\tvalidation_0-mlogloss:1.47992\tvalidation_1-mlogloss:1.49241\n",
            "[25]\tvalidation_0-mlogloss:1.47745\tvalidation_1-mlogloss:1.49065\n",
            "[26]\tvalidation_0-mlogloss:1.47522\tvalidation_1-mlogloss:1.48913\n",
            "[27]\tvalidation_0-mlogloss:1.47306\tvalidation_1-mlogloss:1.48776\n",
            "[28]\tvalidation_0-mlogloss:1.47088\tvalidation_1-mlogloss:1.48622\n",
            "[29]\tvalidation_0-mlogloss:1.46871\tvalidation_1-mlogloss:1.48463\n",
            "[30]\tvalidation_0-mlogloss:1.46655\tvalidation_1-mlogloss:1.48329\n",
            "[31]\tvalidation_0-mlogloss:1.46466\tvalidation_1-mlogloss:1.48183\n",
            "[32]\tvalidation_0-mlogloss:1.46266\tvalidation_1-mlogloss:1.48055\n",
            "[33]\tvalidation_0-mlogloss:1.46081\tvalidation_1-mlogloss:1.47930\n",
            "[34]\tvalidation_0-mlogloss:1.45899\tvalidation_1-mlogloss:1.47827\n",
            "[35]\tvalidation_0-mlogloss:1.45718\tvalidation_1-mlogloss:1.47688\n",
            "[36]\tvalidation_0-mlogloss:1.45554\tvalidation_1-mlogloss:1.47580\n",
            "[37]\tvalidation_0-mlogloss:1.45397\tvalidation_1-mlogloss:1.47484\n",
            "[38]\tvalidation_0-mlogloss:1.45216\tvalidation_1-mlogloss:1.47342\n",
            "[39]\tvalidation_0-mlogloss:1.45054\tvalidation_1-mlogloss:1.47227\n",
            "[40]\tvalidation_0-mlogloss:1.44897\tvalidation_1-mlogloss:1.47144\n",
            "[41]\tvalidation_0-mlogloss:1.44726\tvalidation_1-mlogloss:1.47026\n",
            "[42]\tvalidation_0-mlogloss:1.44569\tvalidation_1-mlogloss:1.46927\n",
            "[43]\tvalidation_0-mlogloss:1.44413\tvalidation_1-mlogloss:1.46833\n",
            "[44]\tvalidation_0-mlogloss:1.44261\tvalidation_1-mlogloss:1.46716\n",
            "[45]\tvalidation_0-mlogloss:1.44118\tvalidation_1-mlogloss:1.46635\n",
            "[46]\tvalidation_0-mlogloss:1.43977\tvalidation_1-mlogloss:1.46563\n",
            "[47]\tvalidation_0-mlogloss:1.43823\tvalidation_1-mlogloss:1.46461\n",
            "[48]\tvalidation_0-mlogloss:1.43678\tvalidation_1-mlogloss:1.46385\n",
            "[49]\tvalidation_0-mlogloss:1.43541\tvalidation_1-mlogloss:1.46309\n",
            "[50]\tvalidation_0-mlogloss:1.43402\tvalidation_1-mlogloss:1.46237\n",
            "[51]\tvalidation_0-mlogloss:1.43256\tvalidation_1-mlogloss:1.46169\n",
            "[52]\tvalidation_0-mlogloss:1.43103\tvalidation_1-mlogloss:1.46089\n",
            "[53]\tvalidation_0-mlogloss:1.42967\tvalidation_1-mlogloss:1.46008\n",
            "[54]\tvalidation_0-mlogloss:1.42809\tvalidation_1-mlogloss:1.45917\n",
            "[55]\tvalidation_0-mlogloss:1.42667\tvalidation_1-mlogloss:1.45880\n",
            "[56]\tvalidation_0-mlogloss:1.42550\tvalidation_1-mlogloss:1.45793\n",
            "[57]\tvalidation_0-mlogloss:1.42422\tvalidation_1-mlogloss:1.45697\n",
            "[58]\tvalidation_0-mlogloss:1.42299\tvalidation_1-mlogloss:1.45635\n",
            "[59]\tvalidation_0-mlogloss:1.42177\tvalidation_1-mlogloss:1.45543\n",
            "[60]\tvalidation_0-mlogloss:1.42046\tvalidation_1-mlogloss:1.45477\n",
            "[61]\tvalidation_0-mlogloss:1.41923\tvalidation_1-mlogloss:1.45409\n",
            "[62]\tvalidation_0-mlogloss:1.41805\tvalidation_1-mlogloss:1.45338\n",
            "[63]\tvalidation_0-mlogloss:1.41673\tvalidation_1-mlogloss:1.45261\n",
            "[64]\tvalidation_0-mlogloss:1.41547\tvalidation_1-mlogloss:1.45209\n",
            "[65]\tvalidation_0-mlogloss:1.41426\tvalidation_1-mlogloss:1.45135\n",
            "[66]\tvalidation_0-mlogloss:1.41306\tvalidation_1-mlogloss:1.45068\n",
            "[67]\tvalidation_0-mlogloss:1.41187\tvalidation_1-mlogloss:1.45009\n",
            "[68]\tvalidation_0-mlogloss:1.41081\tvalidation_1-mlogloss:1.44960\n",
            "[69]\tvalidation_0-mlogloss:1.40975\tvalidation_1-mlogloss:1.44899\n",
            "[70]\tvalidation_0-mlogloss:1.40845\tvalidation_1-mlogloss:1.44855\n",
            "[71]\tvalidation_0-mlogloss:1.40732\tvalidation_1-mlogloss:1.44812\n",
            "[72]\tvalidation_0-mlogloss:1.40624\tvalidation_1-mlogloss:1.44763\n",
            "[73]\tvalidation_0-mlogloss:1.40509\tvalidation_1-mlogloss:1.44707\n",
            "[74]\tvalidation_0-mlogloss:1.40391\tvalidation_1-mlogloss:1.44651\n",
            "[75]\tvalidation_0-mlogloss:1.40283\tvalidation_1-mlogloss:1.44606\n",
            "[76]\tvalidation_0-mlogloss:1.40177\tvalidation_1-mlogloss:1.44551\n",
            "[77]\tvalidation_0-mlogloss:1.40083\tvalidation_1-mlogloss:1.44490\n",
            "[78]\tvalidation_0-mlogloss:1.39979\tvalidation_1-mlogloss:1.44431\n",
            "[79]\tvalidation_0-mlogloss:1.39865\tvalidation_1-mlogloss:1.44377\n",
            "[80]\tvalidation_0-mlogloss:1.39767\tvalidation_1-mlogloss:1.44331\n",
            "[81]\tvalidation_0-mlogloss:1.39654\tvalidation_1-mlogloss:1.44288\n",
            "[82]\tvalidation_0-mlogloss:1.39550\tvalidation_1-mlogloss:1.44236\n",
            "[83]\tvalidation_0-mlogloss:1.39443\tvalidation_1-mlogloss:1.44176\n",
            "[84]\tvalidation_0-mlogloss:1.39341\tvalidation_1-mlogloss:1.44131\n",
            "[85]\tvalidation_0-mlogloss:1.39235\tvalidation_1-mlogloss:1.44086\n",
            "[86]\tvalidation_0-mlogloss:1.39133\tvalidation_1-mlogloss:1.44055\n",
            "[87]\tvalidation_0-mlogloss:1.39027\tvalidation_1-mlogloss:1.43988\n",
            "[88]\tvalidation_0-mlogloss:1.38926\tvalidation_1-mlogloss:1.43945\n",
            "[89]\tvalidation_0-mlogloss:1.38829\tvalidation_1-mlogloss:1.43899\n",
            "[90]\tvalidation_0-mlogloss:1.38735\tvalidation_1-mlogloss:1.43831\n",
            "[91]\tvalidation_0-mlogloss:1.38632\tvalidation_1-mlogloss:1.43798\n",
            "[92]\tvalidation_0-mlogloss:1.38529\tvalidation_1-mlogloss:1.43750\n",
            "[93]\tvalidation_0-mlogloss:1.38439\tvalidation_1-mlogloss:1.43705\n",
            "[94]\tvalidation_0-mlogloss:1.38337\tvalidation_1-mlogloss:1.43648\n",
            "[95]\tvalidation_0-mlogloss:1.38238\tvalidation_1-mlogloss:1.43618\n",
            "[96]\tvalidation_0-mlogloss:1.38145\tvalidation_1-mlogloss:1.43581\n",
            "[97]\tvalidation_0-mlogloss:1.38052\tvalidation_1-mlogloss:1.43518\n",
            "[98]\tvalidation_0-mlogloss:1.37966\tvalidation_1-mlogloss:1.43480\n",
            "[99]\tvalidation_0-mlogloss:1.37874\tvalidation_1-mlogloss:1.43449\n",
            "Evaluating on dev set...\n",
            "Validation Macro F1 Score: 0.3934\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"\\n'n_estimators': [300, 500],\\n'max_depth': [4, 6, 8],\\n'learning_rate': [0.01, 0.05, 0.1],\\n'subsample': [0.7, 0.8, 1.0],\\n'colsample_bytree': [0.7, 0.8, 1.0]\\n\""
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "import xgboost as xgb\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# Load data\n",
        "train_df = pd.read_csv('../data/train.csv')\n",
        "dev_df = pd.read_csv('../data/dev.csv')\n",
        "test_df = pd.read_csv('../data/test.csv')\n",
        "\n",
        "# Preprocessing\n",
        "full_train_texts = train_df['text'].tolist() + dev_df['text'].tolist()\n",
        "full_train_labels = train_df['label'].tolist() + dev_df['label'].tolist()\n",
        "\n",
        "labels = list(sorted(set(full_train_labels)))\n",
        "label2id = {label: idx for idx, label in enumerate(labels)}\n",
        "id2label = {idx: label for label, idx in label2id.items()}\n",
        "\n",
        "y_train = [label2id[label] for label in train_df['label']]\n",
        "y_dev = [label2id[label] for label in dev_df['label']]\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "tfidf = TfidfVectorizer(\n",
        "    analyzer='char_wb',\n",
        "    ngram_range=(2,6),\n",
        "    max_features=50000\n",
        ")\n",
        "\n",
        "print(\"Vectorizing data\")\n",
        "tfidf.fit(full_train_texts)\n",
        "\n",
        "X_train = tfidf.transform(train_df['text'])\n",
        "X_dev = tfidf.transform(dev_df['text'])\n",
        "X_test = tfidf.transform(test_df['text'])\n",
        "print(\"Vectorization complete.\")\n",
        "\n",
        "# Train XGBoost Classifier\n",
        "print(\"Training XGBoost...\")\n",
        "\n",
        "model = xgb.XGBClassifier(\n",
        "    objective='multi:softmax',\n",
        "    num_class=len(labels),\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=3,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    verbosity=1\n",
        ")\n",
        "\n",
        "eval_set = [(X_train, y_train), (X_dev, y_dev)]\n",
        "\n",
        "model.fit(\n",
        "    X_train, y_train,\n",
        "    eval_set=eval_set,\n",
        "    # eval_metric='mlogloss',\n",
        "    # early_stopping_rounds=10,  # Stop if no dev improvement in 10 rounds\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Dev Set Evaluation\n",
        "print(\"Evaluating on dev set...\")\n",
        "dev_preds = model.predict(X_dev)\n",
        "dev_macro_f1 = f1_score(y_dev, dev_preds, average='macro')\n",
        "print(f\"Validation Macro F1 Score: {dev_macro_f1:.4f}\")\n",
        "\n",
        "# # Predict on Test Set\n",
        "# test_preds = model.predict(X_test)\n",
        "# test_labels = [id2label[pred] for pred in test_preds]\n",
        "\n",
        "# # Save submission\n",
        "# output_df = pd.DataFrame({\n",
        "#     'id': test_df['id'],\n",
        "#     'label': test_labels\n",
        "# })\n",
        "# os.makedirs('outputs', exist_ok=True)\n",
        "# output_df.to_csv('outputs/track_3_test.csv', index=False)\n",
        "\n",
        "# print(\"Saved Track 3 submission to outputs/track_3_test.csv\")\n",
        "\n",
        "\"\"\"\n",
        "'n_estimators': [300, 500],\n",
        "'max_depth': [4, 6, 8],\n",
        "'learning_rate': [0.01, 0.05, 0.1],\n",
        "'subsample': [0.7, 0.8, 1.0],\n",
        "'colsample_bytree': [0.7, 0.8, 1.0]\n",
        "\"\"\"\n",
        "\n",
        "# score = 0.3934"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e78c839",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Models not found. Training models...\n",
            "Fitting TF-IDF vectorizer...\n",
            "Training Logistic Regression...\n",
            "Training XGBoost...\n",
            "Generating meta-features...\n",
            "Training meta-model...\n",
            "Stacked Dev Macro F1: 0.4316\n",
            "Saving models...\n",
            "Models saved.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "import xgboost as xgb\n",
        "import joblib\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "SAVE_DIR = \"saved_models\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "# Utility: Check if all models exist\n",
        "def models_exist():\n",
        "    required_files = [\n",
        "        \"tfidf_vectorizer.pkl\",\n",
        "        \"logreg_model.pkl\",\n",
        "        \"xgb_model.pkl\",\n",
        "        \"meta_model.pkl\",\n",
        "        \"label_maps.pkl\"\n",
        "    ]\n",
        "    return all(os.path.exists(os.path.join(SAVE_DIR, fname)) for fname in required_files)\n",
        "\n",
        "# Load Data\n",
        "train_df = pd.read_csv('../data/train.csv')\n",
        "dev_df = pd.read_csv('../data/dev.csv')\n",
        "\n",
        "# Train + Save Models If Not Exist\n",
        "if not models_exist():\n",
        "    print(\"Models not found. Training models...\")\n",
        "\n",
        "    # Label Encoding\n",
        "    all_labels = sorted(set(train_df['label']) | set(dev_df['label']))\n",
        "    label2id = {label: i for i, label in enumerate(all_labels)}\n",
        "    id2label = {i: label for label, i in label2id.items()}\n",
        "    y_train = train_df['label'].map(label2id).values\n",
        "    y_dev = dev_df['label'].map(label2id).values\n",
        "\n",
        "    # TF-IDF Vectorization\n",
        "    print(\"Fitting TF-IDF vectorizer...\")\n",
        "    tfidf = TfidfVectorizer(analyzer='char_wb', ngram_range=(2, 6), max_features=50000)\n",
        "    tfidf.fit(pd.concat([train_df['text'], dev_df['text']]))\n",
        "\n",
        "    X_train = tfidf.transform(train_df['text'])\n",
        "    X_dev = tfidf.transform(dev_df['text'])\n",
        "\n",
        "    # Train base models\n",
        "    print(\"Training Logistic Regression...\")\n",
        "    logreg = LogisticRegression(\n",
        "        penalty='elasticnet',\n",
        "        solver='saga',\n",
        "        max_iter=1000,\n",
        "        C=1.0,\n",
        "        l1_ratio=0.3,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    logreg.fit(X_train, y_train)\n",
        "\n",
        "    print(\"Training XGBoost...\")\n",
        "    xgb_clf = xgb.XGBClassifier(\n",
        "        objective='multi:softprob',\n",
        "        num_class=len(label2id),\n",
        "        n_estimators=300,\n",
        "        learning_rate=0.1,\n",
        "        max_depth=4,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    xgb_clf.fit(X_train, y_train)\n",
        "\n",
        "    # Generate meta-features (dev set)\n",
        "    print(\"Generating meta-features...\")\n",
        "    dev_preds_logreg = logreg.predict_proba(X_dev)\n",
        "    dev_preds_xgb = xgb_clf.predict_proba(X_dev)\n",
        "    X_meta_dev = np.hstack([dev_preds_logreg, dev_preds_xgb])\n",
        "\n",
        "    # Train meta-model\n",
        "    print(\"Training meta-model...\")\n",
        "    meta_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "    meta_model.fit(X_meta_dev, y_dev)\n",
        "\n",
        "    # Evaluate\n",
        "    meta_dev_preds = meta_model.predict(X_meta_dev)\n",
        "    meta_f1 = f1_score(y_dev, meta_dev_preds, average='macro')\n",
        "    print(f\"Stacked Dev Macro F1: {meta_f1:.4f}\")\n",
        "\n",
        "    # Save models\n",
        "    print(\"Saving models...\")\n",
        "    with open(os.path.join(SAVE_DIR, \"tfidf_vectorizer.pkl\"), \"wb\") as f:\n",
        "        pickle.dump(tfidf, f)\n",
        "    joblib.dump(logreg, os.path.join(SAVE_DIR, \"logreg_model.pkl\"))\n",
        "    joblib.dump(xgb_clf, os.path.join(SAVE_DIR, \"xgb_model.pkl\"))\n",
        "    joblib.dump(meta_model, os.path.join(SAVE_DIR, \"meta_model.pkl\"))\n",
        "    with open(os.path.join(SAVE_DIR, \"label_maps.pkl\"), \"wb\") as f:\n",
        "        pickle.dump({'label2id': label2id, 'id2label': id2label}, f)\n",
        "\n",
        "    print(\"Models saved.\") # used 38 mins\n",
        "else:\n",
        "    print(\"Saved models already exist. Skipping training.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ab950bcb",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting TF-IDF on train+dev...\n",
            "Training Logistic Regression on train+dev...\n",
            "Training XGBoost on train+dev...\n",
            "Stacking model predictions on test...\n",
            "Training meta-model on full train+dev predictions...\n",
            "✅ Final submission saved to track_3_test.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import joblib\n",
        "import os\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "import xgboost as xgb\n",
        "\n",
        "# Paths\n",
        "SAVE_DIR = \"saved_models\"\n",
        "TEST_PATH = \"../data/test.csv\"\n",
        "SUBMISSION_PATH = \"track_3_test.csv\"\n",
        "\n",
        "# Load train + dev data\n",
        "train_df = pd.read_csv('../data/train.csv')\n",
        "dev_df = pd.read_csv('../data/dev.csv')\n",
        "test_df = pd.read_csv(TEST_PATH)\n",
        "\n",
        "full_df = pd.concat([train_df, dev_df], ignore_index=True)\n",
        "\n",
        "# Label Encoding\n",
        "all_labels = sorted(full_df['label'].unique())\n",
        "label2id = {label: idx for idx, label in enumerate(all_labels)}\n",
        "id2label = {idx: label for label, idx in label2id.items()}\n",
        "y_full = full_df['label'].map(label2id).values\n",
        "\n",
        "# TF-IDF Vectorizer\n",
        "print(\"Fitting TF-IDF on train+dev...\")\n",
        "tfidf = TfidfVectorizer(analyzer='char_wb', ngram_range=(2, 6), max_features=50000)\n",
        "X_full = tfidf.fit_transform(full_df['text'])\n",
        "X_test = tfidf.transform(test_df['text'])\n",
        "\n",
        "# Logistic Regression\n",
        "print(\"Training Logistic Regression on train+dev...\")\n",
        "logreg = LogisticRegression(\n",
        "    penalty='elasticnet',\n",
        "    solver='saga',\n",
        "    max_iter=1000,\n",
        "    C=1.0,\n",
        "    l1_ratio=0.3,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "logreg.fit(X_full, y_full)\n",
        "\n",
        "# XGBoost\n",
        "print(\"Training XGBoost on train+dev...\")\n",
        "xgb_clf = xgb.XGBClassifier(\n",
        "    objective='multi:softprob',\n",
        "    num_class=len(label2id),\n",
        "    n_estimators=300,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=4,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "xgb_clf.fit(X_full, y_full)\n",
        "\n",
        "# Generate meta-features on test set\n",
        "print(\"Stacking model predictions on test...\")\n",
        "logreg_proba = logreg.predict_proba(X_test)\n",
        "xgb_proba = xgb_clf.predict_proba(X_test)\n",
        "X_meta_test = np.hstack([logreg_proba, xgb_proba])\n",
        "\n",
        "# Meta-model (retrained on train+dev)\n",
        "print(\"Training meta-model on full train+dev predictions...\")\n",
        "# For meta model training, simulate meta features by predicting on X_full (optional if desired)\n",
        "meta_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "meta_model.fit(np.hstack([logreg.predict_proba(X_full), xgb_clf.predict_proba(X_full)]), y_full)\n",
        "\n",
        "# Predict test labels\n",
        "final_preds = meta_model.predict(X_meta_test)\n",
        "final_labels = [id2label[p] for p in final_preds]\n",
        "\n",
        "# Save submission\n",
        "submission_df = pd.DataFrame({\n",
        "    \"id\": test_df[\"id\"],\n",
        "    \"label\": final_labels\n",
        "})\n",
        "submission_df.to_csv(SUBMISSION_PATH, index=False)\n",
        "print(f\"Final submission saved to {SUBMISSION_PATH}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "f0e52818",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final models and vectorizer saved to 'saved_models/'\n"
          ]
        }
      ],
      "source": [
        "# Save models and artifacts\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "# Save TF-IDF vectorizer\n",
        "with open(os.path.join(SAVE_DIR, \"tfidf_vectorizer_final.pkl\"), \"wb\") as f:\n",
        "    pickle.dump(tfidf, f)\n",
        "\n",
        "# Save Logistic Regression\n",
        "joblib.dump(logreg, os.path.join(SAVE_DIR, \"logreg_model_final.pkl\"))\n",
        "\n",
        "# Save XGBoost model\n",
        "joblib.dump(xgb_clf, os.path.join(SAVE_DIR, \"xgb_model_final.pkl\"))\n",
        "\n",
        "# Save Meta model\n",
        "joblib.dump(meta_model, os.path.join(SAVE_DIR, \"meta_model_final.pkl\"))\n",
        "\n",
        "# Save label mappings\n",
        "with open(os.path.join(SAVE_DIR, \"label_maps_final.pkl\"), \"wb\") as f:\n",
        "    pickle.dump({'label2id': label2id, 'id2label': id2label}, f)\n",
        "\n",
        "print(\"Final models and vectorizer saved to 'saved_models/'\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
