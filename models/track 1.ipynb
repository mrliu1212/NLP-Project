{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fa49a84d",
      "metadata": {},
      "source": [
        "# TFIDF + Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "149ee4b7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Macro F1 Score: 0.4114\n",
            "Saved Track 1 submission to outputs/track_1_test.csv\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# Load data\n",
        "train_df = pd.read_csv('../data/train.csv')\n",
        "dev_df = pd.read_csv('../data/dev.csv')\n",
        "test_df = pd.read_csv('../data/test.csv')\n",
        "\n",
        "# Preprocessing\n",
        "\n",
        "# Combine train and dev for vectorizer fitting\n",
        "full_train_texts = train_df['text'].tolist() + dev_df['text'].tolist()\n",
        "full_train_labels = train_df['label'].tolist() + dev_df['label'].tolist()\n",
        "\n",
        "# Mapping labels to ids\n",
        "labels = list(sorted(set(full_train_labels)))\n",
        "label2id = {label: idx for idx, label in enumerate(labels)}\n",
        "id2label = {idx: label for label, idx in label2id.items()}\n",
        "\n",
        "y_train = [label2id[label] for label in train_df['label']]\n",
        "y_dev = [label2id[label] for label in dev_df['label']]\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "tfidf = TfidfVectorizer(\n",
        "    analyzer='char_wb',   # character n-grams inside word boundaries\n",
        "    ngram_range=(2,6),    # 2-6 character n-grams\n",
        "    max_features=50000    # limit vocabulary size\n",
        ")\n",
        "\n",
        "# Fit on training + dev set\n",
        "tfidf.fit(full_train_texts)\n",
        "\n",
        "# Transform\n",
        "X_train = tfidf.transform(train_df['text'])\n",
        "X_dev = tfidf.transform(dev_df['text'])\n",
        "X_test = tfidf.transform(test_df['text'])\n",
        "\n",
        "# Train Logistic Regression\n",
        "clf = LogisticRegression(max_iter=1000, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate on Dev Set\n",
        "dev_preds = clf.predict(X_dev)\n",
        "dev_macro_f1 = f1_score(y_dev, dev_preds, average='macro')\n",
        "print(f\"Validation Macro F1 Score: {dev_macro_f1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5aa6c9c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predict on Test Set\n",
        "test_preds = clf.predict(X_test)\n",
        "test_labels = [id2label[pred] for pred in test_preds]\n",
        "\n",
        "# Save to CSV\n",
        "output_df = pd.DataFrame({\n",
        "    'id': test_df['id'],\n",
        "    'label': test_labels\n",
        "})\n",
        "os.makedirs('outputs', exist_ok=True)\n",
        "output_df.to_csv('outputs/track_1_test.csv', index=False)\n",
        "\n",
        "print(\"Saved Track 1 submission to outputs/track_1_test.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d40a55c3",
      "metadata": {},
      "source": [
        "# TFIDF + Elastic Net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da6802e8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting model\n",
            "Evaluation model\n",
            "Validation Macro F1 Score (retrained best model): 0.4261\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import ParameterGrid, cross_val_score\n",
        "from sklearn.metrics import make_scorer, f1_score\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "# Load data\n",
        "train_df = pd.read_csv('../data/train.csv')\n",
        "dev_df = pd.read_csv('../data/dev.csv')\n",
        "test_df = pd.read_csv('../data/test.csv')\n",
        "\n",
        "# Preprocessing\n",
        "full_train_texts = train_df['text'].tolist() + dev_df['text'].tolist()\n",
        "full_train_labels = train_df['label'].tolist() + dev_df['label'].tolist()\n",
        "\n",
        "labels = list(sorted(set(full_train_labels)))\n",
        "label2id = {label: idx for idx, label in enumerate(labels)}\n",
        "id2label = {idx: label for label, idx in label2id.items()}\n",
        "\n",
        "y_train = [label2id[label] for label in train_df['label']]\n",
        "y_dev = [label2id[label] for label in dev_df['label']]\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "tfidf = TfidfVectorizer(\n",
        "    analyzer='char_wb',\n",
        "    ngram_range=(2,6),\n",
        "    max_features=50000\n",
        ")\n",
        "\n",
        "tfidf.fit(full_train_texts)\n",
        "\n",
        "X_train = tfidf.transform(train_df['text'])\n",
        "X_dev = tfidf.transform(dev_df['text'])\n",
        "X_test = tfidf.transform(test_df['text'])\n",
        "\n",
        "\n",
        "# Too time consuming TODO: Try with cv before submitting\n",
        "# # Hyperparameter Grid\n",
        "# param_grid = {\n",
        "#     'C': [0.01, 10],\n",
        "#     'l1_ratio': [0.3, 0.7]\n",
        "# }\n",
        "# grid = list(ParameterGrid(param_grid))\n",
        "\n",
        "# # Cache file\n",
        "# CACHE_FILE = './cache/ENET_CV5_cache.pkl'\n",
        "\n",
        "# # Load previous cache if exists\n",
        "# if os.path.exists(CACHE_FILE):\n",
        "#     with open(CACHE_FILE, 'rb') as f:\n",
        "#         cache = pickle.load(f)\n",
        "#     print(f\"Loaded {len(cache)} cached results.\")\n",
        "# else:\n",
        "#     cache = {}\n",
        "\n",
        "# best_score = 0\n",
        "# best_params = None\n",
        "\n",
        "# # Define scoring\n",
        "# scorer = make_scorer(f1_score, average='macro')\n",
        "\n",
        "# # Search\n",
        "# for params in tqdm(grid, desc=\"Grid Search with 5-Fold CV\"):\n",
        "#     param_key = tuple(sorted(params.items()))  # make a hashable key\n",
        "\n",
        "#     if param_key in cache:\n",
        "#         macro_f1 = cache[param_key]\n",
        "#     else:\n",
        "#         print(f'Start of parameter tuning: {param_key}')\n",
        "#         model = LogisticRegression(\n",
        "#             penalty='elasticnet',\n",
        "#             solver='saga',\n",
        "#             max_iter=1000,\n",
        "#             random_state=42,\n",
        "#             n_jobs=-1,\n",
        "#             **params\n",
        "#         )\n",
        "#         # Perform 5-Fold CV here!\n",
        "#         scores = cross_val_score(model, X_train, y_train, cv=5, scoring=scorer, n_jobs=-1)\n",
        "#         macro_f1 = np.mean(scores)\n",
        "\n",
        "#         cache[param_key] = macro_f1\n",
        "\n",
        "#         # Save cache every time\n",
        "#         with open(CACHE_FILE, 'wb') as f:\n",
        "#             pickle.dump(cache, f)\n",
        "#         print(f'Cached {param_key}, f1: {macro_f1:.4f}')\n",
        "\n",
        "#     if macro_f1 > best_score:\n",
        "#         best_score = macro_f1\n",
        "#         best_params = params\n",
        "\n",
        "# print(f\"Best parameters: {best_params}\")\n",
        "# print(f\"Best Macro F1 Score (5-Fold CV): {best_score:.4f}\")\n",
        "\n",
        "# Train the best model fully\n",
        "best_model = LogisticRegression(\n",
        "    penalty='elasticnet',\n",
        "    solver='saga',\n",
        "    max_iter=1000,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    C=1,\n",
        "    l1_ratio= 0.3\n",
        "    # **best_params\n",
        ")\n",
        "print(\"Fitting model\")\n",
        "best_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate again on Dev Set\n",
        "print(\"Evaluation model\")\n",
        "dev_preds = best_model.predict(X_dev)\n",
        "dev_macro_f1 = f1_score(y_dev, dev_preds, average='macro')\n",
        "print(f\"Validation Macro F1 Score (retrained best model): {dev_macro_f1:.4f}\")\n",
        "# C=1, L1_ratio=0.7 -> 0.4172\n",
        "# C=1, L1_ratio=0.5 -> 0.4222\n",
        "# C=0.5, L1_ratio=0.5 -> 0.4024\n",
        "# C=1, L1_ratio=0.3 -> 0.4261\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db6dd0bd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predict on Test Set\n",
        "test_preds = clf.predict(X_test)\n",
        "test_labels = [id2label[pred] for pred in test_preds]\n",
        "\n",
        "# Save to CSV\n",
        "output_df = pd.DataFrame({\n",
        "    'id': test_df['id'],\n",
        "    'label': test_labels\n",
        "})\n",
        "os.makedirs('outputs', exist_ok=True)\n",
        "output_df.to_csv('outputs/track_1_test.csv', index=False)\n",
        "\n",
        "print(\"Saved Track 1 submission to outputs/track_1_test.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b136d0f",
      "metadata": {},
      "source": [
        "# Word2Vec + XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "608eb983",
      "metadata": {},
      "outputs": [],
      "source": [
        "import gensim.downloader as api\n",
        "w2v = api.load('word2vec-google-news-300') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "df129979",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 758.5/758.5MB downloaded\n"
          ]
        }
      ],
      "source": [
        "import gensim.downloader as api\n",
        "glove_twitter = api.load('glove-twitter-200')  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "5c298a66",
      "metadata": {},
      "outputs": [],
      "source": [
        "fast_wiki = api.load('fasttext-wiki-news-subwords-300')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bbb16e1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading Word2Vec model...\n",
            "Word2Vec model loaded.\n",
            "Vectorizing texts...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 32000/32000 [00:02<00:00, 11747.66it/s]\n",
            "100%|██████████| 4000/4000 [00:00<00:00, 9016.40it/s]\n",
            "100%|██████████| 4000/4000 [00:00<00:00, 10453.97it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vectorization complete.\n",
            "Training XGBoost...\n",
            "Validation Macro F1 Score: 0.3570\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"\\nn_estimators=300,\\nlearning_rate=0.1,\\nmax_depth=6,\\n-> 0.3628\\n\\nTwitter\\nn_estimators=300,\\nlearning_rate=0.1,\\nmax_depth=6,\\n-> 0.3235\\n\\n\\nn_estimators=500,\\nlearning_rate=0.5,\\nmax_depth=8,\\n-> 0.3645\\n\\n'n_estimators': [300, 500],\\n'max_depth': [4, 6, 8],\\n'learning_rate': [0.01, 0.05, 0.1],\\n'subsample': [0.7, 0.8, 1.0],\\n'colsample_bytree': [0.7, 0.8, 1.0]\\n\""
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "import xgboost as xgb\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# Load data\n",
        "train_df = pd.read_csv('../data/train.csv')\n",
        "dev_df = pd.read_csv('../data/dev.csv')\n",
        "test_df = pd.read_csv('../data/test.csv')\n",
        "\n",
        "# Preprocessing\n",
        "\n",
        "# Combine train and dev for label mapping\n",
        "full_train_labels = train_df['label'].tolist() + dev_df['label'].tolist()\n",
        "\n",
        "# Map labels to integers\n",
        "labels = list(sorted(set(full_train_labels)))\n",
        "label2id = {label: idx for idx, label in enumerate(labels)}\n",
        "id2label = {idx: label for label, idx in label2id.items()}\n",
        "\n",
        "y_train = [label2id[label] for label in train_df['label']]\n",
        "y_dev = [label2id[label] for label in dev_df['label']]\n",
        "\n",
        "# Load pre-trained Word2Vec embeddings\n",
        "print(\"Loading Word2Vec model...\")\n",
        "w2v = fast_wiki # This is 300-dim Google News vectors\n",
        "print(\"Word2Vec model loaded.\")\n",
        "\n",
        "embedding_dim = 300\n",
        "\n",
        "# Function to vectorize text\n",
        "def text_to_vector(text, model, dim):\n",
        "    words = text.split()\n",
        "    word_vectors = []\n",
        "    for word in words:\n",
        "        if word in model:\n",
        "            word_vectors.append(model[word])\n",
        "    if word_vectors:\n",
        "        return np.mean(word_vectors, axis=0)\n",
        "    else:\n",
        "        return np.zeros(dim)\n",
        "\n",
        "# Vectorize train, dev, and test sets\n",
        "print(\"Vectorizing texts...\")\n",
        "\n",
        "X_train = np.vstack([text_to_vector(text, w2v, embedding_dim) for text in tqdm(train_df['text'])])\n",
        "X_dev = np.vstack([text_to_vector(text, w2v, embedding_dim) for text in tqdm(dev_df['text'])])\n",
        "X_test = np.vstack([text_to_vector(text, w2v, embedding_dim) for text in tqdm(test_df['text'])])\n",
        "\n",
        "print(\"Vectorization complete.\")\n",
        "\n",
        "# Train XGBoost Classifier\n",
        "print(\"Training XGBoost...\")\n",
        "\n",
        "model = xgb.XGBClassifier(\n",
        "    objective='multi:softmax',\n",
        "    num_class=len(labels),\n",
        "    n_estimators=300,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=6,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate on Dev Set\n",
        "dev_preds = model.predict(X_dev)\n",
        "dev_macro_f1 = f1_score(y_dev, dev_preds, average='macro')\n",
        "print(f\"Validation Macro F1 Score: {dev_macro_f1:.4f}\")\n",
        "\n",
        "# # Predict on Test Set\n",
        "# test_preds = model.predict(X_test)\n",
        "# test_labels = [id2label[pred] for pred in test_preds]\n",
        "\n",
        "# # Save submission\n",
        "# output_df = pd.DataFrame({\n",
        "#     'id': test_df['id'],\n",
        "#     'label': test_labels\n",
        "# })\n",
        "# os.makedirs('outputs', exist_ok=True)\n",
        "# output_df.to_csv('outputs/track_3_test.csv', index=False)\n",
        "\n",
        "# print(\"Saved Track 3 submission to outputs/track_3_test.csv\")\n",
        "\n",
        "\"\"\"\n",
        "n_estimators=300,\n",
        "learning_rate=0.1,\n",
        "max_depth=6,\n",
        "-> 0.3628\n",
        "\n",
        "n_estimators=500,\n",
        "learning_rate=0.5,\n",
        "max_depth=8,\n",
        "-> 0.3645\n",
        "\n",
        "Twitter\n",
        "n_estimators=300,\n",
        "learning_rate=0.1,\n",
        "max_depth=6,\n",
        "-> 0.3235\n",
        "\n",
        "Fast\n",
        "n_estimators=300,\n",
        "learning_rate=0.1,\n",
        "max_depth=6,\n",
        "-> 0.3570\n",
        "\n",
        "\n",
        "\n",
        "'n_estimators': [300, 500],\n",
        "'max_depth': [4, 6, 8],\n",
        "'learning_rate': [0.01, 0.05, 0.1],\n",
        "'subsample': [0.7, 0.8, 1.0],\n",
        "'colsample_bytree': [0.7, 0.8, 1.0]\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abd4e2c1",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gensim.downloader as api\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import f1_score\n",
        "import xgboost as xgb\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "# Load data\n",
        "train_df = pd.read_csv('../data/train.csv')\n",
        "dev_df = pd.read_csv('../data/dev.csv')\n",
        "test_df = pd.read_csv('../data/test.csv')\n",
        "\n",
        "# Preprocessing\n",
        "\n",
        "full_train_labels = train_df['label'].tolist() + dev_df['label'].tolist()\n",
        "\n",
        "labels = list(sorted(set(full_train_labels)))\n",
        "label2id = {label: idx for idx, label in enumerate(labels)}\n",
        "id2label = {idx: label for idx, label in label2id.items()}\n",
        "\n",
        "y_train = [label2id[label] for label in train_df['label']]\n",
        "y_dev = [label2id[label] for label in dev_df['label']]\n",
        "\n",
        "# Load pre-trained Word2Vec embeddings\n",
        "print(\"Loading Word2Vec model...\")\n",
        "w2v = api.load('word2vec-google-news-300')  # Pretrained 300d embeddings\n",
        "print(\"Word2Vec model loaded.\")\n",
        "\n",
        "embedding_dim = 300\n",
        "\n",
        "# Function to vectorize text\n",
        "def text_to_vector(text, model, dim):\n",
        "    words = text.split()\n",
        "    word_vectors = []\n",
        "    for word in words:\n",
        "        if word in model:\n",
        "            word_vectors.append(model[word])\n",
        "    if word_vectors:\n",
        "        return np.mean(word_vectors, axis=0)\n",
        "    else:\n",
        "        return np.zeros(dim)\n",
        "\n",
        "# Vectorize train, dev, and test sets (with Cache)\n",
        "CACHE_FOLDER = 'cache'\n",
        "os.makedirs(CACHE_FOLDER, exist_ok=True)\n",
        "\n",
        "def get_cached_vectors(filename, texts):\n",
        "    filepath = os.path.join(CACHE_FOLDER, filename)\n",
        "    if os.path.exists(filepath):\n",
        "        print(f\"Loading cached vectors from {filepath}...\")\n",
        "        return np.load(filepath)\n",
        "    else:\n",
        "        print(f\"Vectorizing and caching {filename}...\")\n",
        "        vectors = np.vstack([text_to_vector(text, w2v, embedding_dim) for text in tqdm(texts)])\n",
        "        np.save(filepath, vectors)\n",
        "        return vectors\n",
        "\n",
        "X_train = get_cached_vectors('train_vectors.npy', train_df['text'])\n",
        "X_dev = get_cached_vectors('dev_vectors.npy', dev_df['text'])\n",
        "X_test = get_cached_vectors('test_vectors.npy', test_df['text'])\n",
        "\n",
        "print(\"Vectorization complete.\")\n",
        "\n",
        "# Cache or train best XGBoost model\n",
        "BEST_MODEL_CACHE = os.path.join(CACHE_FOLDER, 'xgb_best_model.pkl')\n",
        "\n",
        "if os.path.exists(BEST_MODEL_CACHE):\n",
        "    print(f\"Loading best model from cache {BEST_MODEL_CACHE}...\")\n",
        "    with open(BEST_MODEL_CACHE, 'rb') as f:\n",
        "        best_model = pickle.load(f)\n",
        "else:\n",
        "    print(\"Starting hyperparameter tuning with GridSearchCV...\")\n",
        "\n",
        "    xgb_model = xgb.XGBClassifier(\n",
        "        objective='multi:softmax',\n",
        "        num_class=len(labels),\n",
        "        random_state=42,\n",
        "        n_jobs=-1,\n",
        "        verbosity=0\n",
        "    )\n",
        "\n",
        "    param_grid = {\n",
        "        'n_estimators': [100, 300],\n",
        "        '   ': [0.05, 0.1],\n",
        "        'max_depth': [4, 6, 8]\n",
        "    }\n",
        "\n",
        "    grid = GridSearchCV(\n",
        "        estimator=xgb_model,\n",
        "        param_grid=param_grid,\n",
        "        cv=5,\n",
        "        scoring='f1_macro',\n",
        "        verbose=2,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    grid.fit(X_train, y_train)\n",
        "\n",
        "    print(f\"Best Params: {grid.best_params_}\")\n",
        "    print(f\"Best CV Macro F1: {grid.best_score_:.4f}\")\n",
        "\n",
        "    best_model = grid.best_estimator_\n",
        "\n",
        "    # Save best model to cache\n",
        "    with open(BEST_MODEL_CACHE, 'wb') as f:\n",
        "        pickle.dump(best_model, f)\n",
        "    print(f\"Saved best model to {BEST_MODEL_CACHE}\")\n",
        "\n",
        "# Evaluate on Dev Set\n",
        "dev_preds = best_model.predict(X_dev)\n",
        "dev_macro_f1 = f1_score(y_dev, dev_preds, average='macro')\n",
        "print(f\"Validation Macro F1 Score (best model): {dev_macro_f1:.4f}\")\n",
        "\n",
        "# Predict on Test Set\n",
        "test_preds = best_model.predict(X_test)\n",
        "test_labels = [id2label[pred] for pred in test_preds]\n",
        "\n",
        "# Save submission\n",
        "output_df = pd.DataFrame({\n",
        "    'id': test_df['id'],\n",
        "    'label': test_labels\n",
        "})\n",
        "os.makedirs('outputs', exist_ok=True)\n",
        "output_df.to_csv('outputs/track_3_test.csv', index=False)\n",
        "\n",
        "print(\"Saved Track 3 submission to outputs/track_3_test.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ba4a31b",
      "metadata": {},
      "source": [
        "# TFIDF + XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8967530",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vectorizing data\n",
            "Vectorization complete.\n",
            "Training XGBoost...\n",
            "[0]\tvalidation_0-mlogloss:1.59660\tvalidation_1-mlogloss:1.59694\n",
            "[1]\tvalidation_0-mlogloss:1.58542\tvalidation_1-mlogloss:1.58588\n",
            "[2]\tvalidation_0-mlogloss:1.57677\tvalidation_1-mlogloss:1.57749\n",
            "[3]\tvalidation_0-mlogloss:1.56816\tvalidation_1-mlogloss:1.56938\n",
            "[4]\tvalidation_0-mlogloss:1.56004\tvalidation_1-mlogloss:1.56188\n",
            "[5]\tvalidation_0-mlogloss:1.55268\tvalidation_1-mlogloss:1.55508\n",
            "[6]\tvalidation_0-mlogloss:1.54618\tvalidation_1-mlogloss:1.54884\n",
            "[7]\tvalidation_0-mlogloss:1.54010\tvalidation_1-mlogloss:1.54338\n",
            "[8]\tvalidation_0-mlogloss:1.53430\tvalidation_1-mlogloss:1.53803\n",
            "[9]\tvalidation_0-mlogloss:1.52921\tvalidation_1-mlogloss:1.53344\n",
            "[10]\tvalidation_0-mlogloss:1.52471\tvalidation_1-mlogloss:1.52969\n",
            "[11]\tvalidation_0-mlogloss:1.52035\tvalidation_1-mlogloss:1.52629\n",
            "[12]\tvalidation_0-mlogloss:1.51612\tvalidation_1-mlogloss:1.52260\n",
            "[13]\tvalidation_0-mlogloss:1.51210\tvalidation_1-mlogloss:1.51908\n",
            "[14]\tvalidation_0-mlogloss:1.50845\tvalidation_1-mlogloss:1.51568\n",
            "[15]\tvalidation_0-mlogloss:1.50501\tvalidation_1-mlogloss:1.51259\n",
            "[16]\tvalidation_0-mlogloss:1.50178\tvalidation_1-mlogloss:1.51010\n",
            "[17]\tvalidation_0-mlogloss:1.49856\tvalidation_1-mlogloss:1.50730\n",
            "[18]\tvalidation_0-mlogloss:1.49551\tvalidation_1-mlogloss:1.50450\n",
            "[19]\tvalidation_0-mlogloss:1.49258\tvalidation_1-mlogloss:1.50206\n",
            "[20]\tvalidation_0-mlogloss:1.49006\tvalidation_1-mlogloss:1.49987\n",
            "[21]\tvalidation_0-mlogloss:1.48742\tvalidation_1-mlogloss:1.49764\n",
            "[22]\tvalidation_0-mlogloss:1.48489\tvalidation_1-mlogloss:1.49552\n",
            "[23]\tvalidation_0-mlogloss:1.48229\tvalidation_1-mlogloss:1.49398\n",
            "[24]\tvalidation_0-mlogloss:1.47992\tvalidation_1-mlogloss:1.49241\n",
            "[25]\tvalidation_0-mlogloss:1.47745\tvalidation_1-mlogloss:1.49065\n",
            "[26]\tvalidation_0-mlogloss:1.47522\tvalidation_1-mlogloss:1.48913\n",
            "[27]\tvalidation_0-mlogloss:1.47306\tvalidation_1-mlogloss:1.48776\n",
            "[28]\tvalidation_0-mlogloss:1.47088\tvalidation_1-mlogloss:1.48622\n",
            "[29]\tvalidation_0-mlogloss:1.46871\tvalidation_1-mlogloss:1.48463\n",
            "[30]\tvalidation_0-mlogloss:1.46655\tvalidation_1-mlogloss:1.48329\n",
            "[31]\tvalidation_0-mlogloss:1.46466\tvalidation_1-mlogloss:1.48183\n",
            "[32]\tvalidation_0-mlogloss:1.46266\tvalidation_1-mlogloss:1.48055\n",
            "[33]\tvalidation_0-mlogloss:1.46081\tvalidation_1-mlogloss:1.47930\n",
            "[34]\tvalidation_0-mlogloss:1.45899\tvalidation_1-mlogloss:1.47827\n",
            "[35]\tvalidation_0-mlogloss:1.45718\tvalidation_1-mlogloss:1.47688\n",
            "[36]\tvalidation_0-mlogloss:1.45554\tvalidation_1-mlogloss:1.47580\n",
            "[37]\tvalidation_0-mlogloss:1.45397\tvalidation_1-mlogloss:1.47484\n",
            "[38]\tvalidation_0-mlogloss:1.45216\tvalidation_1-mlogloss:1.47342\n",
            "[39]\tvalidation_0-mlogloss:1.45054\tvalidation_1-mlogloss:1.47227\n",
            "[40]\tvalidation_0-mlogloss:1.44897\tvalidation_1-mlogloss:1.47144\n",
            "[41]\tvalidation_0-mlogloss:1.44726\tvalidation_1-mlogloss:1.47026\n",
            "[42]\tvalidation_0-mlogloss:1.44569\tvalidation_1-mlogloss:1.46927\n",
            "[43]\tvalidation_0-mlogloss:1.44413\tvalidation_1-mlogloss:1.46833\n",
            "[44]\tvalidation_0-mlogloss:1.44261\tvalidation_1-mlogloss:1.46716\n",
            "[45]\tvalidation_0-mlogloss:1.44118\tvalidation_1-mlogloss:1.46635\n",
            "[46]\tvalidation_0-mlogloss:1.43977\tvalidation_1-mlogloss:1.46563\n",
            "[47]\tvalidation_0-mlogloss:1.43823\tvalidation_1-mlogloss:1.46461\n",
            "[48]\tvalidation_0-mlogloss:1.43678\tvalidation_1-mlogloss:1.46385\n",
            "[49]\tvalidation_0-mlogloss:1.43541\tvalidation_1-mlogloss:1.46309\n",
            "[50]\tvalidation_0-mlogloss:1.43402\tvalidation_1-mlogloss:1.46237\n",
            "[51]\tvalidation_0-mlogloss:1.43256\tvalidation_1-mlogloss:1.46169\n",
            "[52]\tvalidation_0-mlogloss:1.43103\tvalidation_1-mlogloss:1.46089\n",
            "[53]\tvalidation_0-mlogloss:1.42967\tvalidation_1-mlogloss:1.46008\n",
            "[54]\tvalidation_0-mlogloss:1.42809\tvalidation_1-mlogloss:1.45917\n",
            "[55]\tvalidation_0-mlogloss:1.42667\tvalidation_1-mlogloss:1.45880\n",
            "[56]\tvalidation_0-mlogloss:1.42550\tvalidation_1-mlogloss:1.45793\n",
            "[57]\tvalidation_0-mlogloss:1.42422\tvalidation_1-mlogloss:1.45697\n",
            "[58]\tvalidation_0-mlogloss:1.42299\tvalidation_1-mlogloss:1.45635\n",
            "[59]\tvalidation_0-mlogloss:1.42177\tvalidation_1-mlogloss:1.45543\n",
            "[60]\tvalidation_0-mlogloss:1.42046\tvalidation_1-mlogloss:1.45477\n",
            "[61]\tvalidation_0-mlogloss:1.41923\tvalidation_1-mlogloss:1.45409\n",
            "[62]\tvalidation_0-mlogloss:1.41805\tvalidation_1-mlogloss:1.45338\n",
            "[63]\tvalidation_0-mlogloss:1.41673\tvalidation_1-mlogloss:1.45261\n",
            "[64]\tvalidation_0-mlogloss:1.41547\tvalidation_1-mlogloss:1.45209\n",
            "[65]\tvalidation_0-mlogloss:1.41426\tvalidation_1-mlogloss:1.45135\n",
            "[66]\tvalidation_0-mlogloss:1.41306\tvalidation_1-mlogloss:1.45068\n",
            "[67]\tvalidation_0-mlogloss:1.41187\tvalidation_1-mlogloss:1.45009\n",
            "[68]\tvalidation_0-mlogloss:1.41081\tvalidation_1-mlogloss:1.44960\n",
            "[69]\tvalidation_0-mlogloss:1.40975\tvalidation_1-mlogloss:1.44899\n",
            "[70]\tvalidation_0-mlogloss:1.40845\tvalidation_1-mlogloss:1.44855\n",
            "[71]\tvalidation_0-mlogloss:1.40732\tvalidation_1-mlogloss:1.44812\n",
            "[72]\tvalidation_0-mlogloss:1.40624\tvalidation_1-mlogloss:1.44763\n",
            "[73]\tvalidation_0-mlogloss:1.40509\tvalidation_1-mlogloss:1.44707\n",
            "[74]\tvalidation_0-mlogloss:1.40391\tvalidation_1-mlogloss:1.44651\n",
            "[75]\tvalidation_0-mlogloss:1.40283\tvalidation_1-mlogloss:1.44606\n",
            "[76]\tvalidation_0-mlogloss:1.40177\tvalidation_1-mlogloss:1.44551\n",
            "[77]\tvalidation_0-mlogloss:1.40083\tvalidation_1-mlogloss:1.44490\n",
            "[78]\tvalidation_0-mlogloss:1.39979\tvalidation_1-mlogloss:1.44431\n",
            "[79]\tvalidation_0-mlogloss:1.39865\tvalidation_1-mlogloss:1.44377\n",
            "[80]\tvalidation_0-mlogloss:1.39767\tvalidation_1-mlogloss:1.44331\n",
            "[81]\tvalidation_0-mlogloss:1.39654\tvalidation_1-mlogloss:1.44288\n",
            "[82]\tvalidation_0-mlogloss:1.39550\tvalidation_1-mlogloss:1.44236\n",
            "[83]\tvalidation_0-mlogloss:1.39443\tvalidation_1-mlogloss:1.44176\n",
            "[84]\tvalidation_0-mlogloss:1.39341\tvalidation_1-mlogloss:1.44131\n",
            "[85]\tvalidation_0-mlogloss:1.39235\tvalidation_1-mlogloss:1.44086\n",
            "[86]\tvalidation_0-mlogloss:1.39133\tvalidation_1-mlogloss:1.44055\n",
            "[87]\tvalidation_0-mlogloss:1.39027\tvalidation_1-mlogloss:1.43988\n",
            "[88]\tvalidation_0-mlogloss:1.38926\tvalidation_1-mlogloss:1.43945\n",
            "[89]\tvalidation_0-mlogloss:1.38829\tvalidation_1-mlogloss:1.43899\n",
            "[90]\tvalidation_0-mlogloss:1.38735\tvalidation_1-mlogloss:1.43831\n",
            "[91]\tvalidation_0-mlogloss:1.38632\tvalidation_1-mlogloss:1.43798\n",
            "[92]\tvalidation_0-mlogloss:1.38529\tvalidation_1-mlogloss:1.43750\n",
            "[93]\tvalidation_0-mlogloss:1.38439\tvalidation_1-mlogloss:1.43705\n",
            "[94]\tvalidation_0-mlogloss:1.38337\tvalidation_1-mlogloss:1.43648\n",
            "[95]\tvalidation_0-mlogloss:1.38238\tvalidation_1-mlogloss:1.43618\n",
            "[96]\tvalidation_0-mlogloss:1.38145\tvalidation_1-mlogloss:1.43581\n",
            "[97]\tvalidation_0-mlogloss:1.38052\tvalidation_1-mlogloss:1.43518\n",
            "[98]\tvalidation_0-mlogloss:1.37966\tvalidation_1-mlogloss:1.43480\n",
            "[99]\tvalidation_0-mlogloss:1.37874\tvalidation_1-mlogloss:1.43449\n",
            "Evaluating on dev set...\n",
            "Validation Macro F1 Score: 0.3934\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"\\n'n_estimators': [300, 500],\\n'max_depth': [4, 6, 8],\\n'learning_rate': [0.01, 0.05, 0.1],\\n'subsample': [0.7, 0.8, 1.0],\\n'colsample_bytree': [0.7, 0.8, 1.0]\\n\""
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "import xgboost as xgb\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# Load data\n",
        "train_df = pd.read_csv('../data/train.csv')\n",
        "dev_df = pd.read_csv('../data/dev.csv')\n",
        "test_df = pd.read_csv('../data/test.csv')\n",
        "\n",
        "# Preprocessing\n",
        "full_train_texts = train_df['text'].tolist() + dev_df['text'].tolist()\n",
        "full_train_labels = train_df['label'].tolist() + dev_df['label'].tolist()\n",
        "\n",
        "labels = list(sorted(set(full_train_labels)))\n",
        "label2id = {label: idx for idx, label in enumerate(labels)}\n",
        "id2label = {idx: label for label, idx in label2id.items()}\n",
        "\n",
        "y_train = [label2id[label] for label in train_df['label']]\n",
        "y_dev = [label2id[label] for label in dev_df['label']]\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "tfidf = TfidfVectorizer(\n",
        "    analyzer='char_wb',\n",
        "    ngram_range=(2,6),\n",
        "    max_features=50000\n",
        ")\n",
        "\n",
        "print(\"Vectorizing data\")\n",
        "tfidf.fit(full_train_texts)\n",
        "\n",
        "X_train = tfidf.transform(train_df['text'])\n",
        "X_dev = tfidf.transform(dev_df['text'])\n",
        "X_test = tfidf.transform(test_df['text'])\n",
        "print(\"Vectorization complete.\")\n",
        "\n",
        "# Train XGBoost Classifier\n",
        "print(\"Training XGBoost...\")\n",
        "\n",
        "model = xgb.XGBClassifier(\n",
        "    objective='multi:softmax',\n",
        "    num_class=len(labels),\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=3,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    verbosity=1\n",
        ")\n",
        "\n",
        "eval_set = [(X_train, y_train), (X_dev, y_dev)]\n",
        "\n",
        "model.fit(\n",
        "    X_train, y_train,\n",
        "    eval_set=eval_set,\n",
        "    # eval_metric='mlogloss',\n",
        "    # early_stopping_rounds=10,  # ⛔ Stop if no dev improvement in 10 rounds\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Dev Set Evaluation\n",
        "print(\"Evaluating on dev set...\")\n",
        "dev_preds = model.predict(X_dev)\n",
        "dev_macro_f1 = f1_score(y_dev, dev_preds, average='macro')\n",
        "print(f\"Validation Macro F1 Score: {dev_macro_f1:.4f}\")\n",
        "\n",
        "# # Predict on Test Set\n",
        "# test_preds = model.predict(X_test)\n",
        "# test_labels = [id2label[pred] for pred in test_preds]\n",
        "\n",
        "# # Save submission\n",
        "# output_df = pd.DataFrame({\n",
        "#     'id': test_df['id'],\n",
        "#     'label': test_labels\n",
        "# })\n",
        "# os.makedirs('outputs', exist_ok=True)\n",
        "# output_df.to_csv('outputs/track_3_test.csv', index=False)\n",
        "\n",
        "# print(\"Saved Track 3 submission to outputs/track_3_test.csv\")\n",
        "\n",
        "\"\"\"\n",
        "'n_estimators': [300, 500],\n",
        "'max_depth': [4, 6, 8],\n",
        "'learning_rate': [0.01, 0.05, 0.1],\n",
        "'subsample': [0.7, 0.8, 1.0],\n",
        "'colsample_bytree': [0.7, 0.8, 1.0]\n",
        "\"\"\"\n",
        "\n",
        "# score = 0.3934"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e78c839",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Models not found. Training models...\n",
            "Fitting TF-IDF vectorizer...\n",
            "Training Logistic Regression...\n",
            "Training XGBoost...\n",
            "Generating meta-features...\n",
            "Training meta-model...\n",
            "Stacked Dev Macro F1: 0.4316\n",
            "Saving models...\n",
            "Models saved.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "import xgboost as xgb\n",
        "import joblib\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "SAVE_DIR = \"saved_models\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "# ========== Utility: Check if all models exist ==========\n",
        "def models_exist():\n",
        "    required_files = [\n",
        "        \"tfidf_vectorizer.pkl\",\n",
        "        \"logreg_model.pkl\",\n",
        "        \"xgb_model.pkl\",\n",
        "        \"meta_model.pkl\",\n",
        "        \"label_maps.pkl\"\n",
        "    ]\n",
        "    return all(os.path.exists(os.path.join(SAVE_DIR, fname)) for fname in required_files)\n",
        "\n",
        "# ========== Load Data ==========\n",
        "train_df = pd.read_csv('../data/train.csv')\n",
        "dev_df = pd.read_csv('../data/dev.csv')\n",
        "\n",
        "# ========== Train + Save Models If Not Exist ==========\n",
        "if not models_exist():\n",
        "    print(\"Models not found. Training models...\")\n",
        "\n",
        "    # Label Encoding\n",
        "    all_labels = sorted(set(train_df['label']) | set(dev_df['label']))\n",
        "    label2id = {label: i for i, label in enumerate(all_labels)}\n",
        "    id2label = {i: label for label, i in label2id.items()}\n",
        "    y_train = train_df['label'].map(label2id).values\n",
        "    y_dev = dev_df['label'].map(label2id).values\n",
        "\n",
        "    # TF-IDF Vectorization\n",
        "    print(\"Fitting TF-IDF vectorizer...\")\n",
        "    tfidf = TfidfVectorizer(analyzer='char_wb', ngram_range=(2, 6), max_features=50000)\n",
        "    tfidf.fit(pd.concat([train_df['text'], dev_df['text']]))\n",
        "\n",
        "    X_train = tfidf.transform(train_df['text'])\n",
        "    X_dev = tfidf.transform(dev_df['text'])\n",
        "\n",
        "    # Train base models\n",
        "    print(\"Training Logistic Regression...\")\n",
        "    logreg = LogisticRegression(\n",
        "        penalty='elasticnet',\n",
        "        solver='saga',\n",
        "        max_iter=1000,\n",
        "        C=1.0,\n",
        "        l1_ratio=0.3,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    logreg.fit(X_train, y_train)\n",
        "\n",
        "    print(\"Training XGBoost...\")\n",
        "    xgb_clf = xgb.XGBClassifier(\n",
        "        objective='multi:softprob',\n",
        "        num_class=len(label2id),\n",
        "        n_estimators=300,\n",
        "        learning_rate=0.1,\n",
        "        max_depth=4,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    xgb_clf.fit(X_train, y_train)\n",
        "\n",
        "    # Generate meta-features (dev set)\n",
        "    print(\"Generating meta-features...\")\n",
        "    dev_preds_logreg = logreg.predict_proba(X_dev)\n",
        "    dev_preds_xgb = xgb_clf.predict_proba(X_dev)\n",
        "    X_meta_dev = np.hstack([dev_preds_logreg, dev_preds_xgb])\n",
        "\n",
        "    # Train meta-model\n",
        "    print(\"Training meta-model...\")\n",
        "    meta_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "    meta_model.fit(X_meta_dev, y_dev)\n",
        "\n",
        "    # Evaluate\n",
        "    meta_dev_preds = meta_model.predict(X_meta_dev)\n",
        "    meta_f1 = f1_score(y_dev, meta_dev_preds, average='macro')\n",
        "    print(f\"Stacked Dev Macro F1: {meta_f1:.4f}\")\n",
        "\n",
        "    # Save models\n",
        "    print(\"Saving models...\")\n",
        "    with open(os.path.join(SAVE_DIR, \"tfidf_vectorizer.pkl\"), \"wb\") as f:\n",
        "        pickle.dump(tfidf, f)\n",
        "    joblib.dump(logreg, os.path.join(SAVE_DIR, \"logreg_model.pkl\"))\n",
        "    joblib.dump(xgb_clf, os.path.join(SAVE_DIR, \"xgb_model.pkl\"))\n",
        "    joblib.dump(meta_model, os.path.join(SAVE_DIR, \"meta_model.pkl\"))\n",
        "    with open(os.path.join(SAVE_DIR, \"label_maps.pkl\"), \"wb\") as f:\n",
        "        pickle.dump({'label2id': label2id, 'id2label': id2label}, f)\n",
        "\n",
        "    print(\"Models saved.\") # used 38 mins\n",
        "else:\n",
        "    print(\"Saved models already exist. Skipping training.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
