{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a38942a6",
   "metadata": {
    "id": "a38942a6"
   },
   "source": [
    "## Assignment 3: Subreddit Classification\n",
    "\n",
    "In this assignment, you will train and evaluate text classification models.\n",
    "Your goal is to classify Reddit comments according to which subreddit they were posted in.\n",
    "\n",
    "\n",
    "### üìö Data\n",
    "\n",
    "The data you will be working with consists of real-world text comments from Reddit. \n",
    "Comments are sampled at equal proportions from each of five political subreddits: The_Donald, Conservative, politics, Libertarian, and ChapoTrapHouse.\n",
    "\n",
    "Here is an example entry: \n",
    "- `id`: 2091\n",
    "- `text`: Eating soap to own the republicans\n",
    "- `label`: ChapoTrapHouse\n",
    "\n",
    "The data is split into three files.\n",
    "The TRAIN and DEV sets contain texts and labels. Use these to develop your classification models.\n",
    "The TEST set contains only texts. This is what you will be assessed on.\n",
    "\n",
    "Please note that these are real Reddit comments.\n",
    "We filtered out likely-inappropriate content, but some comments may still be sensitive or offensive.\n",
    "\n",
    "\n",
    "### üìù Task\n",
    "\n",
    "Your task is to train a model to classify posts according to which subreddit they belong to.\n",
    "This is a five-way classification task with balanced data.\n",
    "\n",
    "\n",
    "### ‚öôÔ∏è Implementation\n",
    "\n",
    "There will be 3 Tracks.\n",
    "Each track corresponds to a different type of model you will train.\n",
    "\n",
    "- **Track 1 - \"classic ML\"**: Logistic regression, SVM, random forest, or any other model that does not require training a neural network. You can still use neural embeddings (e.g. GloVe, Word2Vec) as input features.\n",
    "- **Track 2 - \"BERT-style\"**: Encoder models like DistilBERT, RoBERTa, DeBERTa, etc. You can use pre-trained models, but you must fine-tune them on the training data.\n",
    "- **Track 3 (‚ú®OPTIONAL‚ú®): -  \"Open\"**: Any classifier. This could include methods not covered in class. It is important that you provide a clear description of the methodology you used in your submission.\n",
    "\n",
    "You may **not** use additional data for training your model.\n",
    "You may **not** use API models (e.g. GPT-4, Claude, etc.) either for processing inputs (e.g. getting embeddings) or for getting predictions.\n",
    "\n",
    "### üìà Baselines\n",
    "\n",
    "For your reference, we provide the following baselines:\n",
    "\n",
    "#### TF-IDF Logistic Regression Classifier with 2-6 Character Ngrams\n",
    "Trained on TRAIN and evaluated on DEV.\n",
    "\n",
    "| Class           | Precision | Recall | F1-score | Support |\n",
    "|-----------------|-----------|--------|----------|---------|\n",
    "| ChapoTrapHouse  | 0.45      | 0.55   | 0.50     | 799     |\n",
    "| Libertarian     | 0.52      | 0.52   | 0.52     | 798     |\n",
    "| politics        | 0.39      | 0.34   | 0.36     | 799     |\n",
    "| Conservative    | 0.32      | 0.26   | 0.29     | 802     |\n",
    "| The_Donald      | 0.39      | 0.44   | 0.41     | 802     |\n",
    "| Accuracy        |           |        | 0.42     |         |\n",
    "| Macro Avg       | 0.42      | 0.42   | 0.42     | 4000    |\n",
    "| Weighted Avg    | 0.42      | 0.42   | 0.42     | 4000    |\n",
    "\n",
    "#### Random Choice Classifier\n",
    "Assigns class labels to predictions entirely randomly, without considering any input features or patterns in the data.\n",
    "\n",
    "| Class           | Precision | Recall | F1-score | Support |\n",
    "|-----------------|-----------|--------|----------|---------|\n",
    "| ChapoTrapHouse  | 0.20      | 0.20   | 0.20     | 799     |\n",
    "| Libertarian     | 0.20      | 0.21   | 0.20     | 798     |\n",
    "| politics        | 0.19      | 0.19   | 0.19     | 799     |\n",
    "| Conservative    | 0.20      | 0.19   | 0.20     | 802     |\n",
    "| The_Donald      | 0.19      | 0.20   | 0.19     | 802     |\n",
    "| Accuracy        |           |        | 0.20     |         |\n",
    "| Macro Avg       | 0.20      | 0.20   | 0.20     | 4000    |\n",
    "| Weighted Avg    | 0.20      | 0.20   | 0.20     | 4000    |\n",
    "\n",
    "### üèÖ Assessment\n",
    "\n",
    "Tracks will have equal weighting in the final grade.\n",
    "If you submit to Track 3, **we will choose the best two tracks to evaluate your assignment**.\n",
    "\n",
    "For each track, you will be assessed based on the **Macro F1 Score** of your predictions on the TEST set.\n",
    "We have provided an example for how to calculate Macro F1 below, which you should use in developing your classification model using the TRAIN and DEV sets.\n",
    "We will use this exact implementation to evaluate your predictions on the TEST set.\n",
    "\n",
    "For each track, your submission will be a CSV file with two columns:\n",
    "`id`, which is the ID of the comment in the TEST set, and\n",
    "`label`, which is the text label you predict for that comment (e.g. \"politics\" or \"ChapoTrapHouse\").\n",
    "\n",
    "You also have to submit a brief description of the methodology you used for each track (max 100 words per track).\n",
    "It is very important that you stick to the \"allowed\" methods for each track.\n",
    "We will check your code: if it is missing or does not conform to the regulations, you will receive a 0 for that track.\n",
    "\n",
    "\n",
    "### üì• Submission Instructions\n",
    "\n",
    "Follow these instructions to submit your assigment on BlackBoard:\n",
    "\n",
    "1. **File structure**: Ensure that your submission is a .zip file, and that it contains the following items with exactly these specified names:\n",
    "  - `track_1_test.csv`: A CSV file with two columns (id, label) for Track 1.\n",
    "  - `track_2_test.csv`: A CSV file with two columns (id, label) for Track 2.\n",
    "  - `track_3_test.csv` (optional): A CSV file with two columns (id, label) for Track 3.\n",
    "  - `description.txt`: A brief description of the methodology you used for each track (max 100 words per track).\n",
    "  - `/code`: A folder containing all your code for the assignment. This code needs to be well-documented, and fully and easily reproducible by us. For very large files, include a Google Drive link to the files in your description.txt instead of uploading them directly.\n",
    "2. **Submission**: Upload the .zip file to the BlackBoard Assignment 3 section.\n",
    "3. **Deadline**: Please refer to BlackBoard.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78414d3d",
   "metadata": {
    "id": "78414d3d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of Macro F1 Score calculation\n",
    "# we will use this implementation to evaluate your submissions\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "y_true = [\"ChapoTrapHouse\", \"politics\", \"ChapoTrapHouse\"]\n",
    "y_pred = [\"ChapoTrapHouse\", \"politics\", \"politics\"]\n",
    "\n",
    "f1_score(y_true, y_pred, average=\"macro\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "sandbox",
   "language": "python",
   "name": "sandbox"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
