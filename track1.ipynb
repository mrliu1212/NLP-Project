{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d042f12",
      "metadata": {
        "id": "1d042f12",
        "outputId": "d072537a-b652-449a-b7a8-122386037e7c"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import f1_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 1. Load data\n",
        "# ----------------\n",
        "# Read CSVs into pandas DataFrames. 'test' has no labels.\n",
        "train = pd.read_csv(\"train.csv\")  # Training set with labels\n",
        "dev = pd.read_csv(\"dev.csv\")      # Development set with labels\n",
        "test = pd.read_csv(\"test.csv\")    # Test set without labels\n",
        "\n",
        "# 2. Vectorize text\n",
        "# --------------------\n",
        "# Convert raw text into TF-IDF feature vectors.\n",
        "# - ngram_range=(2,6): consider character n-grams of length 2 to 6.\n",
        "# - min_df=5: ignore infrequent n-grams to reduce noise.\n",
        "vectorizer = TfidfVectorizer(ngram_range=(2,6), min_df=5)\n",
        "X_train = vectorizer.fit_transform(train['text'])\n",
        "y_train = train['label']\n",
        "X_dev = vectorizer.transform(dev['text'])\n",
        "y_dev = dev['label']\n",
        "X_test = vectorizer.transform(test['text'])\n",
        "\n",
        "# 3. Hyperparameter tuning with progress bar\n",
        "# --------------------------------------------\n",
        "# Define grid of Random Forest hyperparameters:\n",
        "# - n_estimators: number of trees in the forest\n",
        "# - max_depth: maximum depth of each tree\n",
        "# - max_features: number of features to consider at each split\n",
        "\n",
        "# param_grid = {\n",
        "#     'n_estimators': [100, 200, 500],\n",
        "#     'max_depth': [None, 10, 20],\n",
        "#     'max_features': ['sqrt', 'log2', 0.3]\n",
        "# }\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': 200,\n",
        "    'max_depth': None,\n",
        "    'max_features': 'sqrt'\n",
        "}\n",
        "\n",
        "# Create a list of all parameter combinations\n",
        "import itertools\n",
        "param_list = list(itertools.product(\n",
        "    param_grid['n_estimators'],\n",
        "    param_grid['max_depth'],\n",
        "    param_grid['max_features']\n",
        "))\n",
        "\n",
        "best_score = 0\n",
        "best_params = None\n",
        "print(\"Tuning Random Forest hyperparameters:\")\n",
        "for n_est, depth, feat in tqdm(param_list, desc=\"Grid Search\", unit=\"combo\"):\n",
        "    # Initialize a Random Forest with current hyperparameters\n",
        "    rf = RandomForestClassifier(\n",
        "        n_estimators=n_est,\n",
        "        max_depth=depth,\n",
        "        max_features=feat,\n",
        "        oob_score=False,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    # 3-fold cross-validation for macro-F1 on TRAIN\n",
        "    scores = cross_val_score(\n",
        "        rf, X_train, y_train,\n",
        "        scoring='f1_macro',\n",
        "        cv=3,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    mean_score = scores.mean()\n",
        "    # Update best parameters if current is better\n",
        "    if mean_score > best_score:\n",
        "        best_score = mean_score\n",
        "        best_params = {'n_estimators': n_est, 'max_depth': depth, 'max_features': feat}\n",
        "\n",
        "print(f\"\\nBest params: {best_params} with CV macro-F1 = {best_score:.4f}\")\n",
        "\n",
        "# 4. Train best Random Forest on full TRAIN set\n",
        "# ------------------------------------------------\n",
        "best_rf = RandomForestClassifier(\n",
        "    **best_params,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "best_rf.fit(X_train, y_train)\n",
        "\n",
        "# 5. Evaluate on DEV set\n",
        "# -----------------------\n",
        "dev_preds = best_rf.predict(X_dev)\n",
        "print(\"DEV Macro F1:\", f1_score(y_dev, dev_preds, average='macro'))\n",
        "\n",
        "# 6. (Optional) Retrain on TRAIN + DEV and predict TEST\n",
        "# ------------------------------------------------------\n",
        "# all_texts = pd.concat([train['text'], dev['text']])\n",
        "# all_labels = pd.concat([train['label'], dev['label']])\n",
        "# X_all = vectorizer.fit_transform(all_texts)\n",
        "# best_rf.fit(X_all, all_labels)\n",
        "# test_preds = best_rf.predict(X_test)\n",
        "# submission = pd.DataFrame({'id': test['id'], 'label': test_preds})\n",
        "# submission.to_csv('track_1_test.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "640cc164",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "640cc164",
        "outputId": "da013629-0e8f-4dc3-8388-15d367ff5577"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'train.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-039014fb16d8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# 1. Load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mdev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dev.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 1. Load data\n",
        "train = pd.read_csv(\"train.csv\")\n",
        "dev = pd.read_csv(\"dev.csv\")\n",
        "test = pd.read_csv(\"test.csv\")\n",
        "\n",
        "# 2. Vectorize text\n",
        "vectorizer = TfidfVectorizer(ngram_range=(2,6), min_df=5)\n",
        "X_train = vectorizer.fit_transform(train['text'])\n",
        "y_train = train['label']\n",
        "X_dev = vectorizer.transform(dev['text'])\n",
        "y_dev = dev['label']\n",
        "\n",
        "# 3. Define a single set of RF hyperparameters\n",
        "# We'll start with a common default: 200 trees, no max depth, sqrt features\n",
        "rf_params = {\n",
        "    'n_estimators': 200,\n",
        "    'max_depth': None,\n",
        "    'max_features': 'sqrt'\n",
        "}\n",
        "print(f\"Training Random Forest with params: {rf_params}\")\n",
        "\n",
        "# 4. Quick cross-validation to estimate performance\n",
        "rf = RandomForestClassifier(\n",
        "    **rf_params,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "# 3-fold CV on training set\n",
        "cv_scores = cross_val_score(rf, X_train, y_train, scoring='f1_macro', cv=3, n_jobs=-1)\n",
        "print(f\"CV Macro F1 scores: {cv_scores}\")\n",
        "print(f\"Mean CV Macro F1: {cv_scores.mean():.4f}\")\n",
        "\n",
        "# 5. Train on full training set and evaluate on DEV set\n",
        "rf.fit(X_train, y_train)\n",
        "dev_preds = rf.predict(X_dev)\n",
        "print(\"DEV Macro F1:\", f1_score(y_dev, dev_preds, average='macro'))\n",
        "\n",
        "# 6. (Optional) Retrain on TRAIN+DEV and predict TEST\n",
        "# all_texts = pd.concat([train['text'], dev['text']])\n",
        "# all_labels = pd.concat([train['label'], dev['label']])\n",
        "# X_all = vectorizer.fit_transform(all_texts)\n",
        "# rf.fit(X_all, all_labels)\n",
        "# test_preds = rf.predict(vectorizer.transform(test['text']))\n",
        "# submission = pd.DataFrame({'id': test['id'], 'label': test_preds})\n",
        "# submission.to_csv('track_1_test.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92d25634",
      "metadata": {
        "id": "92d25634",
        "outputId": "d332e970-2a3b-4428-d052-8ef8bbd5d3c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tuning regularization strength C:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Grid Search - C values: 100%|██████████| 4/4 [00:09<00:00,  2.49s/value]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Best C found: 1 with CV macro-F1 = 0.3164\n",
            "DEV Macro F1: 0.3149562265024328\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import f1_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 1. Load data\n",
        "# ----------------\n",
        "train = pd.read_csv(\"train.csv\")  # Training set\n",
        "dev = pd.read_csv(\"dev.csv\")      # Development/validation set\n",
        "test = pd.read_csv(\"test.csv\")    # Test set (unlabeled)\n",
        "\n",
        "# 2. Vectorize text\n",
        "# --------------------\n",
        "vectorizer = TfidfVectorizer(ngram_range=(2,6), min_df=5)\n",
        "X_train = vectorizer.fit_transform(train['text'])\n",
        "y_train = train['label']\n",
        "X_dev = vectorizer.transform(dev['text'])\n",
        "y_dev = dev['label']\n",
        "X_test = vectorizer.transform(test['text'])\n",
        "\n",
        "# 3. Manual hyperparameter tuning with progress bar\n",
        "# ---------------------------------------------------\n",
        "# Define grid of regularization strengths\n",
        "param_C = [0.01, 0.1, 1, 10]\n",
        "best_score = 0\n",
        "best_C = None\n",
        "print(\"Tuning regularization strength C:\")\n",
        "for C in tqdm(param_C, desc=\"Grid Search - C values\", unit=\"value\"):\n",
        "    # Initialize model with current C\n",
        "    lr = LogisticRegression(\n",
        "        penalty='l2',\n",
        "        solver='saga',\n",
        "        max_iter=1000,\n",
        "        C=C,\n",
        "        n_jobs=-1,\n",
        "        random_state=42\n",
        "    )\n",
        "    # Perform 3-fold CV on training set, scoring macro-F1\n",
        "    scores = cross_val_score(\n",
        "        lr, X_train, y_train,\n",
        "        scoring='f1_macro',\n",
        "        cv=3,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    mean_score = scores.mean()\n",
        "    if mean_score > best_score:\n",
        "        best_score = mean_score\n",
        "        best_C = C\n",
        "        best_scores = scores\n",
        "\n",
        "print(f\"\\nBest C found: {best_C} with CV macro-F1 = {best_score:.4f}\")\n",
        "\n",
        "# Train best model on full TRAIN\n",
        "best_lr = LogisticRegression(\n",
        "    penalty='l2',\n",
        "    solver='saga',\n",
        "    max_iter=1000,\n",
        "    C=best_C,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "best_lr.fit(X_train, y_train)\n",
        "\n",
        "# 4. Evaluate on DEV set\n",
        "# -----------------------\n",
        "dev_preds = best_lr.predict(X_dev)\n",
        "print(\"DEV Macro F1:\", f1_score(y_dev, dev_preds, average='macro'))\n",
        "\n",
        "# 5. (Optional) Retrain on TRAIN + DEV and predict TEST\n",
        "# ------------------------------------------------------\n",
        "# all_texts = pd.concat([train['text'], dev['text']])\n",
        "# all_labels = pd.concat([train['label'], dev['label']])\n",
        "# X_all = vectorizer.fit_transform(all_texts)\n",
        "# best_lr.fit(X_all, all_labels)\n",
        "# test_preds = best_lr.predict(X_test)\n",
        "\n",
        "# # Save submission\n",
        "# submission = pd.DataFrame({'id': test['id'], 'label': test_preds})\n",
        "# submission.to_csv('track_1_test.csv', index=False)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}