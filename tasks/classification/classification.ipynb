{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "K3INfWGMUSDf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3INfWGMUSDf",
        "outputId": "e906e3b6-1618-4a00-fcb6-e12c7c2abcc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mhint: Using 'master' as the name for the initial branch. This default branch name\u001b[m\n",
            "\u001b[33mhint: is subject to change. To configure the initial branch name to use in all\u001b[m\n",
            "\u001b[33mhint: of your new repositories, which will suppress this warning, call:\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: \tgit config --global init.defaultBranch <name>\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: Names commonly chosen instead of 'master' are 'main', 'trunk' and\u001b[m\n",
            "\u001b[33mhint: 'development'. The just-created branch can be renamed via this command:\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: \tgit branch -m <name>\u001b[m\n",
            "Initialized empty Git repository in /content/.git/\n"
          ]
        }
      ],
      "source": [
        "!git init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "drqQRl8DUHx1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drqQRl8DUHx1",
        "outputId": "96af3ac6-5b8b-4ba0-9a1f-e81f85d58c04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Branch 'media-bias' set up to track remote branch 'media-bias' from 'origin'.\n",
            "Switched to a new branch 'media-bias'\n"
          ]
        }
      ],
      "source": [
        "!git checkout media-bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "DLDMu2hnUbQ_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLDMu2hnUbQ_",
        "outputId": "f7c08a67-8d59-4b61-84cd-f64da75442af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/tasks/classification\n"
          ]
        }
      ],
      "source": [
        "%cd ./tasks/classification/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7812320",
      "metadata": {
        "id": "c7812320"
      },
      "source": [
        "# Data Loading & Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e7e5d56c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7e5d56c",
        "outputId": "57784817-efcc-4e4f-8dfe-bb55f776c3ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded cache from '../../data/train_data_35_5000.json' (127631 items).\n",
            "Loaded cache from '../../data/test_data_20_5000.json' (82494 items).\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))\n",
        "if parent_dir not in sys.path:\n",
        "    sys.path.insert(0, parent_dir)\n",
        "\n",
        "import pandas as pd\n",
        "from useful_tools import UsefulTools\n",
        "\n",
        "training_file_name = 'train_data_35_5000'\n",
        "\n",
        "# LOAD DATA\n",
        "training_data = pd.DataFrame(UsefulTools.JsonCache.load(f'../../data/{training_file_name}.json', expected_type=list))\n",
        "test_data = pd.DataFrame(UsefulTools.JsonCache.load('../../data/test_data_20_5000.json', expected_type=list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "988f940d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "988f940d",
        "outputId": "a6e610cd-164e-4a63-c97c-46f8c6ea908d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Selected top sources: ['Forbes', 'SiliconANGLE News', 'Plos.org', 'Business Insider', 'The Verge', 'Dwarkesh.com', 'GlobeNewswire', 'Gizmodo.com', 'Securityaffairs.com', 'Yahoo Entertainment']\n",
            "Training data (n sentences):\n",
            "[['Forbes' 11041]\n",
            " ['SiliconANGLE News' 5744]\n",
            " ['Plos.org' 3967]\n",
            " ['Business Insider' 3820]\n",
            " ['The Verge' 3685]\n",
            " ['Dwarkesh.com' 3562]\n",
            " ['GlobeNewswire' 3378]\n",
            " ['Gizmodo.com' 3274]\n",
            " ['Securityaffairs.com' 3042]\n",
            " ['Yahoo Entertainment' 2837]]\n",
            "\n",
            "Test data (n sentences):\n",
            "[['SiliconANGLE News' 11097]\n",
            " ['Plos.org' 7974]\n",
            " ['Forbes' 5307]\n",
            " ['GlobeNewswire' 4991]\n",
            " ['Dwarkesh.com' 1781]\n",
            " ['Business Insider' 1592]\n",
            " ['Securityaffairs.com' 1341]\n",
            " ['The Verge' 1313]\n",
            " ['Yahoo Entertainment' 1171]\n",
            " ['Gizmodo.com' 824]]\n",
            "\n",
            "Training sources: ['Gizmodo.com' 'Yahoo Entertainment' 'The Verge' 'Business Insider'\n",
            " 'Forbes' 'Securityaffairs.com' 'SiliconANGLE News' 'GlobeNewswire'\n",
            " 'Dwarkesh.com' 'Plos.org']\n",
            "Test sources: ['The Verge' 'Gizmodo.com' 'Forbes' 'Yahoo Entertainment' 'GlobeNewswire'\n",
            " 'Plos.org' 'Securityaffairs.com' 'Business Insider' 'Dwarkesh.com'\n",
            " 'SiliconANGLE News']\n",
            "Training samples: 44350\n",
            "Test samples: 37391\n"
          ]
        }
      ],
      "source": [
        "# PROCESS DATA\n",
        "# Count sentences per source\n",
        "source_counts = training_data['source'].value_counts()\n",
        "\n",
        "# Get the top 4 sources with highest sentence count\n",
        "top_sources = source_counts.head(10).index\n",
        "\n",
        "# Filter both datasets to include only these top 4 sources\n",
        "training_data = training_data[training_data['source'].isin(top_sources)].reset_index(drop=True)\n",
        "test_data = test_data[test_data['source'].isin(top_sources)].reset_index(drop=True)\n",
        "\n",
        "print(f\"\\nSelected top sources: {list(top_sources)}\")\n",
        "print(f\"Training data (n sentences):\\n{training_data[['id','source']].groupby('source').count().reset_index().sort_values(by='id',ascending=False).values}\")\n",
        "print(f\"\\nTest data (n sentences):\\n{test_data[['id','source']].groupby('source').count().reset_index().sort_values(by='id',ascending=False).values}\")\n",
        "print(f\"\\nTraining sources: {training_data.source.unique()}\")\n",
        "print(f\"Test sources: {test_data.source.unique()}\")\n",
        "print(f\"Training samples: {len(training_data)}\")\n",
        "print(f\"Test samples: {len(test_data)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "820306c7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "820306c7",
        "outputId": "e2dc3d32-88d5-40f6-fca1-5a6af9fbeccb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running cross-validation grid search...\n",
            "Loaded HalvingGridSearchCV from cache: ./cache/halving_logreg.pkl\n",
            "Best Params: {'C': 2.0, 'l1_ratio': 0.3}\n",
            "Best Score: 0.5378\n",
            "Fitting best model...\n",
            "Evaluating model on dev set...\n",
            "Validation Macro F1 Score: 0.4743\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "from useful_tools import UsefulTools\n",
        "\n",
        "# 1. Preprocessing\n",
        "full_train_texts = training_data['sentence'].tolist() + test_data['sentence'].tolist()\n",
        "full_train_labels = training_data['source'].tolist() + test_data['source'].tolist()\n",
        "\n",
        "labels = list(sorted(set(full_train_labels)))\n",
        "label2id = {label: idx for idx, label in enumerate(labels)}\n",
        "id2label = {idx: label for label, idx in label2id.items()}\n",
        "\n",
        "y_train = [label2id[label] for label in training_data['source']]\n",
        "y_dev = [label2id[label] for label in test_data['source']]\n",
        "\n",
        "# 2. TF-IDF Vectorization\n",
        "tfidf = TfidfVectorizer(\n",
        "    analyzer='char_wb',\n",
        "    ngram_range=(2, 6),\n",
        "    max_features=50000\n",
        ")\n",
        "\n",
        "tfidf.fit(full_train_texts)\n",
        "X_train = tfidf.transform(training_data['sentence'])\n",
        "X_dev = tfidf.transform(test_data['sentence'])\n",
        "\n",
        "# 3. Hyperparameter Tuning with CVGridSearch\n",
        "param_grid = {\n",
        "    'C': [0.1, 0.5, 1.0, 1.5, 2.0, 5],\n",
        "    'l1_ratio': [0.1, 0.3, 0.5, 0.7]\n",
        "}\n",
        "fixed_params = {\n",
        "    'penalty': 'elasticnet',\n",
        "    'solver': 'saga',\n",
        "    'max_iter': 5000,\n",
        "    'random_state': 42,\n",
        "    'n_jobs': -1,\n",
        "    'class_weight': 'balanced'\n",
        "}\n",
        "\n",
        "model = LogisticRegression(**fixed_params)\n",
        "halving_search = UsefulTools.HalvingGridSearch(\n",
        "    estimator=model,\n",
        "    param_grid=param_grid,\n",
        "    scoring='f1_macro',\n",
        "    cv=3,\n",
        "    factor=2,\n",
        "    cache_file='./cache/halving_logreg.pkl',\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"Running cross-validation grid search...\")\n",
        "best_model, best_params, best_score = halving_search.search_and_fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# Final Model Training\n",
        "enet = LogisticRegression(**fixed_params, **best_params)\n",
        "print(\"Fitting best model...\")\n",
        "enet.fit(X_train, y_train)\n",
        "\n",
        "# Evaluation\n",
        "print(\"Evaluating model on dev set...\")\n",
        "dev_preds = enet.predict(X_dev)\n",
        "dev_macro_f1 = f1_score(y_dev, dev_preds, average='macro')\n",
        "print(f\"Validation Macro F1 Score: {dev_macro_f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "23f62770",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "23f62770",
        "outputId": "1bc68b9a-294f-4d57-8887-d5acbf949822"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted Source: The Verge\n",
            "\n",
            "Top Predictions:\n",
            "  The Verge       : 0.4787\n",
            "  Forbes          : 0.3099\n",
            "  Dwarkesh.com    : 0.1131\n",
            "  SiliconANGLE News : 0.0983\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The Verge'"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def predict_custom_sentence(sentence: str, top_k: int = 4):\n",
        "    # Vectorize the input sentence\n",
        "    vectorized = tfidf.transform([sentence])\n",
        "\n",
        "    # Predict class ID and get probabilities\n",
        "    probas = enet.predict_proba(vectorized)[0]\n",
        "    pred_id = np.argmax(probas)\n",
        "    pred_label = id2label[pred_id]\n",
        "\n",
        "    # Sort and show top_k class probabilities\n",
        "    top_indices = np.argsort(probas)[::-1][:top_k]\n",
        "    # print(f\"\\nInput Sentence: {sentence}\")\n",
        "    print(f\"Predicted Source: {pred_label}\")\n",
        "    print(\"\\nTop Predictions:\")\n",
        "    for idx in top_indices:\n",
        "        print(f\"  {id2label[idx]:<15} : {probas[idx]:.4f}\")\n",
        "\n",
        "    return pred_label\n",
        "\n",
        "\n",
        "# Example usage\n",
        "sample = \"Making sure that a user is who they say they are is at the core of San Francisco-based Persona, which offers ID authentication software to 3,000 companies including OpenAI, LinkedIn, Etsy, Reddit, DoorDash and Robinhood.\"\n",
        "predict_custom_sentence(sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a09014fc",
      "metadata": {
        "id": "a09014fc"
      },
      "source": [
        "# roBERTa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "c52d2436",
      "metadata": {
        "id": "c52d2436"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW  # <-- Correct import here!\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification, get_linear_schedule_with_warmup\n",
        "from sklearn.metrics import f1_score\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# PyTorch-ready dataset\n",
        "class RedditDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length):\n",
        "        self.texts = texts              # Store the list of comment texts\n",
        "        self.labels = labels            # Store the corresponding list of integer labels\n",
        "        self.tokenizer = tokenizer      # Store the tokenizer (e.g., RobertaTokenizer)\n",
        "        self.max_length = max_length    # Store the maximum sequence length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)  # Important: this allows DataLoader to know dataset size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Fetches a single sample from the dataset.\n",
        "\n",
        "        Args:\n",
        "            idx (int): Index of the item.\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary containing:\n",
        "                - 'input_ids': Raw text must be converted into numbers because models can't understand plain words.\n",
        "                - 'attention_mask': A list that tells the model which tokens are real (1) and which ones are just padding (0).\n",
        "                - 'labels': The label\n",
        "\n",
        "        * Tensor is a multi-dimensional array (like a super-powered NumPy array) that can run on a GPU very efficiently\n",
        "\n",
        "        ex:\n",
        "        Raw Text:\n",
        "        \"eating soap to own the republicans\"\n",
        "\n",
        "        Tokenized to IDs:\n",
        "        [0, 1553, 4153, 7, 1225, 5, 13815, 2, *1, *1]\n",
        "\n",
        "        Attention Mask:\n",
        "        [1, 1, 1, 1, 1, 1, 1, 1, *0, *0]\n",
        "\n",
        "        Label:\n",
        "        0\n",
        "\n",
        "        Models usually expect inputs to have the same length (max = max_lenght).\n",
        "        If your token list is too short, you add padding tokens (which are just zeros).\n",
        "        \"\"\"\n",
        "        # Fetch the text and its corresponding label using the index\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Tokenize the text:\n",
        "        # - truncation: cuts off texts longer than max_length\n",
        "        # - padding: adds padding tokens to shorter texts\n",
        "        # - return_tensors='pt': returns PyTorch tensors instead of lists\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length = self.max_length,\n",
        "            return_tensors='pt' # stands for PyTorch\n",
        "        )\n",
        "\n",
        "        # Return a dictionary with model inputs and label\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(),         # Tensor of input token IDs .squeeze() again just removes the extra dimension.\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(), # Tensor indicating real tokens vs padding\n",
        "            'labels': torch.tensor(label, dtype=torch.long)        # Make sure it becomes a PyTorch Tensor, because the model expects labels to be Tensors during training.\n",
        "        }\n",
        "\n",
        "\n",
        "from torch.amp import GradScaler, autocast\n",
        "# Use smaller (half-size) numbers for most calculations, so training is much faster and uses less memory — without losing much accuracy.\n",
        "scaler = GradScaler()\n",
        "\n",
        "# Mixed Precision Training\n",
        "def train_MPT(model, loader):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        with autocast(device_type=device.type):  # 🔥 autocast for mixed precision\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "# Evaluation Function\n",
        "def evaluate(model, loader):\n",
        "    model.eval() # Set the model to evaluation mode\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad(): # Turns off gradient calculations (Saves memory and speeds up evaluation because we don't need gradients)\n",
        "        for batch in tqdm(loader):\n",
        "            # Move data to device\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            # Forward pass\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            # Get predictions (pick the class with the highest score (probability) for each example in the batch)\n",
        "            preds = torch.argmax(outputs.logits, dim=1) # outputs.logits = raw scores for each class\n",
        "            # Save predictions and true labels\n",
        "            # We Move predictions and labels from GPU to CPU and convert them into numpy arrays\n",
        "            # because some Python operations (like NumPy or scikit-learn) cannot work directly with GPU Tensors.\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Take the F1 score for each label (class) separately, and then average them equally (It does not care if one label has 1000 examples and another class has 100 examples)\n",
        "    macro_f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    return macro_f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "c0912771",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0912771",
        "outputId": "1c27d9db-b1a4-43d7-b7ee-e14ea9357871"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No saved model found. Starting training...\n",
            "Epoch 1/2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1045/1045 [06:21<00:00,  2.74it/s]\n",
            "100%|██████████| 210/210 [01:27<00:00,  2.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 3.1052, Validation Macro F1: 0.0314\n",
            "Epoch 2/2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1045/1045 [06:17<00:00,  2.77it/s]\n",
            "100%|██████████| 210/210 [01:27<00:00,  2.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 2.2538, Validation Macro F1: 0.0368\n",
            "Best f1 for this model: 0.03684385149134057\n",
            "Training finished. Best model saved to cache/train_data_15_50_best_roberta-base_model_128-64-2-2e-05.pt\n"
          ]
        }
      ],
      "source": [
        "# 1. Preprocessing\n",
        "labels = training_data['source'].unique()\n",
        "label2id = {label: idx for idx, label in enumerate(labels)} # {\"label\": 'id'}\n",
        "id2label = {idx: label for label, idx in label2id.items()} # {'id':'label}\n",
        "\n",
        "\n",
        "training_data['label_id'] = training_data['source'].map(label2id)\n",
        "test_data['label_id'] = test_data['source'].map(label2id)\n",
        "\n",
        "\n",
        "MODEL_NAME = 'roberta-base'\n",
        "MAX_LENGTH = 128\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 2\n",
        "LEARNING_RATE = 2e-5\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "training_data['label_id'] = training_data['source'].map(label2id)\n",
        "test_data['label_id'] = test_data['source'].map(label2id)\n",
        "\n",
        "train_dataset = RedditDataset(training_data['sentence'].tolist(), training_data['label_id'].tolist(), tokenizer, MAX_LENGTH)\n",
        "dev_dataset = RedditDataset(test_data['sentence'].tolist(), test_data['label_id'].tolist(), tokenizer, MAX_LENGTH)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "# Model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # cuda is faster but it requires GPU\n",
        "model = RobertaForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(labels))\n",
        "model.to(device) # Moves all model weights and computations to the device you selected (cuda if GPU, otherwise cpu)\n",
        "\n",
        "# Optimizer and Scheduler\n",
        "# model.parameters() are all the learnable weights in the model.\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE) # A type of optimizer that updates the model's weights to minimize the loss.\n",
        "total_steps = len(train_loader) * EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps) # It controls how the learning rate changes during training. In our case it linearly decreases\n",
        "\n",
        "\n",
        "MODEL_SAVE_PATH = f'cache/{training_file_name}_best_{MODEL_NAME}_model_{MAX_LENGTH}-{BATCH_SIZE}-{EPOCHS}-{LEARNING_RATE}.pt'\n",
        "\n",
        "# Check if saved model exists\n",
        "if os.path.exists(MODEL_SAVE_PATH):\n",
        "    print(f\"Loading saved model from {MODEL_SAVE_PATH}...\")\n",
        "    model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
        "else:\n",
        "    print(\"No saved model found. Starting training...\")\n",
        "\n",
        "    # Run Training\n",
        "    best_f1 = 0\n",
        "    best_model_state = None\n",
        "\n",
        "    for epoch in range(EPOCHS): # One epoch = One full pass through the entire training dataset.\n",
        "        print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
        "        train_loss = train_MPT(model, train_loader)\n",
        "        val_f1 = evaluate(model, dev_loader)\n",
        "        print(f\"Train Loss: {train_loss:.4f}, Validation Macro F1: {val_f1:.4f}\")\n",
        "\n",
        "        if val_f1 > best_f1:\n",
        "            best_f1 = val_f1\n",
        "            best_model_state = model.state_dict()\n",
        "\n",
        "    print(f'Best f1 for this model: {best_f1}')\n",
        "\n",
        "    # After training, save the best model\n",
        "    torch.save(best_model_state, MODEL_SAVE_PATH)\n",
        "    print(f\"Training finished. Best model saved to {MODEL_SAVE_PATH}\")\n",
        "\n",
        "    # Loads back the best version of the model that you saved during training (Forget your current weights — load these saved weights instead)\n",
        "    model.load_state_dict(best_model_state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a59bb8fe",
      "metadata": {
        "id": "a59bb8fe"
      },
      "outputs": [],
      "source": [
        "def predict_sentence_roberta(text):\n",
        "    model.eval()\n",
        "    inputs = tokenizer(\n",
        "        text,\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        max_length=MAX_LENGTH,\n",
        "        return_tensors='pt'\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        probs = torch.softmax(outputs.logits, dim=1).cpu().numpy().flatten()\n",
        "        pred_idx = np.argmax(probs)\n",
        "        print(f\"\\n Sentence: {text}\")\n",
        "        print(f\"Predicted Source: {id2label[pred_idx]}\")\n",
        "        print(\"\\n Top Probabilities:\")\n",
        "        for idx in probs.argsort()[::-1]:\n",
        "            print(f\"  {id2label[idx]:<15} : {probs[idx]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IF-icxmUaFmK",
      "metadata": {
        "id": "IF-icxmUaFmK"
      },
      "source": [
        "# Stacking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "09106owzaJi0",
      "metadata": {
        "id": "09106owzaJi0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "class StackedNewsClassifier:\n",
        "    def __init__(self, label2id, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "        self.label2id = label2id\n",
        "        self.id2label = {v: k for k, v in label2id.items()}\n",
        "        self.num_classes = len(label2id)\n",
        "\n",
        "        self.device = torch.device(device)\n",
        "\n",
        "        # Components\n",
        "        self.tfidf = None\n",
        "        self.lr_model = None\n",
        "        self.roberta_tokenizer = None\n",
        "        self.roberta_model = None\n",
        "\n",
        "    def fit_tfidf_logreg(self, texts, labels, **tfidf_kwargs):\n",
        "        self.tfidf = TfidfVectorizer(\n",
        "            analyzer='char_wb',\n",
        "            ngram_range=(2, 6),\n",
        "            max_features=50000,\n",
        "            **tfidf_kwargs\n",
        "        )\n",
        "        X = self.tfidf.fit_transform(texts)\n",
        "        y = [self.label2id[label] for label in labels]\n",
        "\n",
        "        self.lr_model = LogisticRegression(\n",
        "            penalty='elasticnet',\n",
        "            solver='saga',\n",
        "            max_iter=5000,\n",
        "            random_state=42,\n",
        "            n_jobs=-1,\n",
        "            C=1.0,\n",
        "            l1_ratio=0.3,\n",
        "            class_weight='balanced'\n",
        "        )\n",
        "        self.lr_model.fit(X, y)\n",
        "\n",
        "    def load_roberta(self, model_path, model_name='roberta-base'):\n",
        "        self.roberta_tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
        "        self.roberta_model = RobertaForSequenceClassification.from_pretrained(\n",
        "            model_name, num_labels=self.num_classes\n",
        "        )\n",
        "        self.roberta_model.load_state_dict(torch.load(model_path))\n",
        "        self.roberta_model.to(self.device)\n",
        "        self.roberta_model.eval()\n",
        "\n",
        "    def predict_proba(self, sentence, max_length=128, alpha=0.5):\n",
        "        \"\"\"\n",
        "        Combines TF-IDF + LR and RoBERTa predictions (weighted average).\n",
        "        \"\"\"\n",
        "        # TF-IDF + Logistic Regression\n",
        "        X = self.tfidf.transform([sentence])\n",
        "        prob_lr = self.lr_model.predict_proba(X)[0]\n",
        "\n",
        "        # RoBERTa\n",
        "        inputs = self.roberta_tokenizer(\n",
        "            sentence,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=max_length,\n",
        "            return_tensors='pt'\n",
        "        ).to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = self.roberta_model(**inputs).logits\n",
        "            prob_roberta = F.softmax(logits, dim=1).cpu().numpy()[0]\n",
        "\n",
        "        # Combine: weighted average\n",
        "        combined_prob = alpha * prob_roberta + (1 - alpha) * prob_lr\n",
        "        return combined_prob\n",
        "\n",
        "    def predict(self, sentence, max_length=128, alpha=0.5, top_k=3):\n",
        "        prob = self.predict_proba(sentence, max_length=max_length, alpha=alpha)\n",
        "        pred_idx = np.argmax(prob)\n",
        "        print(f\"\\n📝 Sentence: {sentence}\")\n",
        "        print(f\"📢 Predicted Source: {self.id2label[pred_idx]}\")\n",
        "\n",
        "        print(f\"\\n🔍 Top {top_k} Probabilities:\")\n",
        "        for idx in prob.argsort()[::-1][:top_k]:\n",
        "            print(f\"  {self.id2label[idx]:<15}: {prob[idx]:.4f}\")\n",
        "\n",
        "        return self.id2label[pred_idx]\n",
        "\n",
        "    def evaluate(self, sentences, true_labels, alpha=0.5, max_length=128):\n",
        "        \"\"\"\n",
        "        Evaluate the stacked model on a list of sentences and true labels.\n",
        "\n",
        "        Args:\n",
        "            sentences (list[str]): List of raw input texts.\n",
        "            true_labels (list[str] or list[int]): Ground-truth labels (string or integer).\n",
        "            alpha (float): Weight for RoBERTa in the ensemble.\n",
        "            max_length (int): Token truncation/padding length for RoBERTa.\n",
        "\n",
        "        Returns:\n",
        "            float: Macro F1 score\n",
        "        \"\"\"\n",
        "        self.roberta_model.eval()\n",
        "        y_true = [self.label2id[label] if isinstance(label, str) else label for label in true_labels]\n",
        "        y_pred = []\n",
        "\n",
        "        for text in tqdm(sentences, desc=\"Evaluating\"):\n",
        "            probs = self.predict_proba(text, alpha=alpha, max_length=max_length)\n",
        "            pred_idx = np.argmax(probs)\n",
        "            y_pred.append(pred_idx)\n",
        "\n",
        "        macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
        "        print(f\"\\n✅ Macro F1 Score: {macro_f1:.4f}\")\n",
        "        return macro_f1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "5z1jIN-vaYwi",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5z1jIN-vaYwi",
        "outputId": "6f95e40b-6fc0-4e48-d1b0-553e6d92057e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|██████████| 438/438 [00:05<00:00, 83.70it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✅ Macro F1 Score: 0.3470\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.34703113350405895"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 1. Setup\n",
        "stacker = StackedNewsClassifier(label2id)\n",
        "\n",
        "# 2. Train TF-IDF + LR model\n",
        "stacker.fit_tfidf_logreg(\n",
        "    texts=training_data['sentence'].tolist(),\n",
        "    labels=training_data['source'].tolist()\n",
        ")\n",
        "\n",
        "# 3. Load pre-trained RoBERTa model\n",
        "stacker.load_roberta(model_path='cache/best_roberta-base_model_128-64-2-2e-05.pt')\n",
        "\n",
        "stacker.evaluate(\n",
        "    sentences=test_data['sentence'].tolist(),\n",
        "    true_labels=test_data['label_id'].tolist(),  # or test_data['source'] if strings\n",
        "    alpha=0.6  # weight more toward RoBERTa\n",
        ")\n",
        "\n",
        "\n",
        "# # 4. Predict\n",
        "# stacker.predict(\"The prime minister held a press briefing on new trade policies.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "Z8llCidJapUz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "Z8llCidJapUz",
        "outputId": "a3c84ab3-fcb0-434c-e846-0c6e2f0b51ca"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📝 Sentence: The prime minister held a press briefing on new trade policies.\n",
            "📢 Predicted Source: The Verge\n",
            "\n",
            "🔍 Top 3 Probabilities:\n",
            "  The Verge      : 0.5655\n",
            "  Business Insider: 0.1762\n",
            "  Gizmodo.com    : 0.1420\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The Verge'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 4. Predict\n",
        "stacker.predict(\"The prime minister held a press briefing on new trade policies.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
