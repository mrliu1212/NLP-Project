{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7812320",
   "metadata": {},
   "source": [
    "# Data Loading & Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e7e5d56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded cache from '../../data/train_data_15_50.json' (66818 items).\n",
      "Loaded cache from '../../data/test_data_10_50.json' (32453 items).\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)\n",
    "\n",
    "import pandas as pd\n",
    "from useful_tools import UsefulTools\n",
    "\n",
    "# LOAD DATA\n",
    "training_data = pd.DataFrame(UsefulTools.JsonCache.load('../../data/train_data_15_50.json', expected_type=list))\n",
    "test_data = pd.DataFrame(UsefulTools.JsonCache.load('../../data/test_data_10_50.json', expected_type=list))\n",
    "\n",
    "\n",
    "# PROCESS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "988f940d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected top sources: ['SiliconANGLE News', 'Forbes', 'The Verge', 'Dwarkesh.com']\n",
      "Training data (n sentences):\n",
      "[['SiliconANGLE News' 7015]\n",
      " ['Forbes' 5889]\n",
      " ['The Verge' 3831]\n",
      " ['Dwarkesh.com' 3562]]\n",
      "\n",
      "Test data (n sentences):\n",
      "[['Forbes' 5653]\n",
      " ['The Verge' 1629]]\n",
      "\n",
      "Training sources: ['The Verge' 'Forbes' 'SiliconANGLE News' 'Dwarkesh.com']\n",
      "Test sources: ['The Verge' 'Forbes']\n",
      "Training samples: 20297\n",
      "Test samples: 7282\n"
     ]
    }
   ],
   "source": [
    "# Count sentences per source\n",
    "source_counts = training_data['source'].value_counts()\n",
    "\n",
    "# Get the top 4 sources with highest sentence count\n",
    "top_sources = source_counts.head(4).index\n",
    "\n",
    "# Filter both datasets to include only these top 4 sources\n",
    "training_data = training_data[training_data['source'].isin(top_sources)].reset_index(drop=True)\n",
    "test_data = test_data[test_data['source'].isin(top_sources)].reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nSelected top sources: {list(top_sources)}\")\n",
    "print(f\"Training data (n sentences):\\n{training_data[['id','source']].groupby('source').count().reset_index().sort_values(by='id',ascending=False).values}\")\n",
    "print(f\"\\nTest data (n sentences):\\n{test_data[['id','source']].groupby('source').count().reset_index().sort_values(by='id',ascending=False).values}\")\n",
    "print(f\"\\nTraining sources: {training_data.source.unique()}\")\n",
    "print(f\"Test sources: {test_data.source.unique()}\")\n",
    "print(f\"Training samples: {len(training_data)}\")\n",
    "print(f\"Test samples: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "820306c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running cross-validation grid search...\n",
      "üîÅ Loaded HalvingGridSearchCV from cache: ./cache/halving_logreg.pkl\n",
      "üèÜ Best Params: {'C': 2.0, 'l1_ratio': 0.3}\n",
      "üìà Best Score: 0.5378\n",
      "Fitting best model...\n",
      "Evaluating model on dev set...\n",
      "Validation Macro F1 Score: 0.3929\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from useful_tools import UsefulTools\n",
    "\n",
    "# 1. Preprocessing\n",
    "full_train_texts = training_data['sentence'].tolist() + test_data['sentence'].tolist()\n",
    "full_train_labels = training_data['source'].tolist() + test_data['source'].tolist()\n",
    "\n",
    "labels = list(sorted(set(full_train_labels)))\n",
    "label2id = {label: idx for idx, label in enumerate(labels)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "y_train = [label2id[label] for label in training_data['source']]\n",
    "y_dev = [label2id[label] for label in test_data['source']]\n",
    "\n",
    "# 2. TF-IDF Vectorization\n",
    "tfidf = TfidfVectorizer(\n",
    "    analyzer='char_wb',\n",
    "    ngram_range=(2, 6),\n",
    "    max_features=50000\n",
    ")\n",
    "\n",
    "tfidf.fit(full_train_texts)\n",
    "X_train = tfidf.transform(training_data['sentence'])\n",
    "X_dev = tfidf.transform(test_data['sentence'])\n",
    "\n",
    "# 3. Hyperparameter Tuning with CVGridSearch\n",
    "param_grid = {\n",
    "    'C': [0.1, 0.5, 1.0, 1.5, 2.0, 5],\n",
    "    'l1_ratio': [0.1, 0.3, 0.5, 0.7]\n",
    "}\n",
    "fixed_params = {\n",
    "    'penalty': 'elasticnet',\n",
    "    'solver': 'saga',\n",
    "    'max_iter': 5000,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "    'class_weight': 'balanced'\n",
    "}\n",
    "\n",
    "model = LogisticRegression(**fixed_params)\n",
    "halving_search = UsefulTools.HalvingGridSearch(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    scoring='f1_macro',\n",
    "    cv=3,\n",
    "    factor=2,\n",
    "    cache_file='./cache/halving_logreg.pkl',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Running cross-validation grid search...\")\n",
    "best_model, best_params, best_score = halving_search.search_and_fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# 4. Final Model Training\n",
    "enet = LogisticRegression(**fixed_params, **best_params)\n",
    "print(\"Fitting best model...\")\n",
    "enet.fit(X_train, y_train)\n",
    "\n",
    "# 5. Evaluation\n",
    "print(\"Evaluating model on dev set...\")\n",
    "dev_preds = enet.predict(X_dev)\n",
    "dev_macro_f1 = f1_score(y_dev, dev_preds, average='macro')\n",
    "print(f\"Validation Macro F1 Score: {dev_macro_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6eacaf45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on dev set...\n",
      "Validation Macro F1 Score: 0.3929\n"
     ]
    }
   ],
   "source": [
    "# 5. Evaluation\n",
    "X_dev = tfidf.transform(test_data['sentence'])\n",
    "print(\"Evaluating model on dev set...\")\n",
    "dev_preds = enet.predict(X_dev)\n",
    "dev_macro_f1 = f1_score(y_dev, dev_preds, average='macro')\n",
    "print(f\"Validation Macro F1 Score: {dev_macro_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "23f62770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Source: The Verge\n",
      "\n",
      "Top Predictions:\n",
      "  The Verge       : 0.4796\n",
      "  Forbes          : 0.3074\n",
      "  Dwarkesh.com    : 0.1162\n",
      "  SiliconANGLE News : 0.0967\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The Verge'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict_custom_sentence(sentence: str, top_k: int = 4):\n",
    "    # Vectorize the input sentence\n",
    "    vectorized = tfidf.transform([sentence])\n",
    "\n",
    "    # Predict class ID and get probabilities\n",
    "    probas = enet.predict_proba(vectorized)[0]\n",
    "    pred_id = np.argmax(probas)\n",
    "    pred_label = id2label[pred_id]\n",
    "\n",
    "    # Sort and show top_k class probabilities\n",
    "    top_indices = np.argsort(probas)[::-1][:top_k]\n",
    "    # print(f\"\\nInput Sentence: {sentence}\")\n",
    "    print(f\"Predicted Source: {pred_label}\")\n",
    "    print(\"\\nTop Predictions:\")\n",
    "    for idx in top_indices:\n",
    "        print(f\"  {id2label[idx]:<15} : {probas[idx]:.4f}\")\n",
    "\n",
    "    return pred_label\n",
    "\n",
    "\n",
    "# Example usage\n",
    "sample = \"Making sure that a user is who they say they are is at the core of San Francisco-based Persona, which offers ID authentication software to 3,000 companies including OpenAI, LinkedIn, Etsy, Reddit, DoorDash and Robinhood.\"\n",
    "predict_custom_sentence(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09014fc",
   "metadata": {},
   "source": [
    "# roBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52d2436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW  # <-- Correct import here!\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# PyTorch-ready dataset\n",
    "class RedditDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts              # Store the list of comment texts\n",
    "        self.labels = labels            # Store the corresponding list of integer labels\n",
    "        self.tokenizer = tokenizer      # Store the tokenizer (e.g., RobertaTokenizer)\n",
    "        self.max_length = max_length    # Store the maximum sequence length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)  # Important: this allows DataLoader to know dataset size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Fetches a single sample from the dataset.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the item.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing:\n",
    "                - 'input_ids': Raw text must be converted into numbers because models can't understand plain words.\n",
    "                - 'attention_mask': A list that tells the model which tokens are real (1) and which ones are just padding (0).\n",
    "                - 'labels': The label\n",
    "\n",
    "        * Tensor is a multi-dimensional array (like a super-powered NumPy array) that can run on a GPU very efficiently\n",
    "\n",
    "        ex:\n",
    "        Raw Text:\n",
    "        \"eating soap to own the republicans\"\n",
    "\n",
    "        Tokenized to IDs:\n",
    "        [0, 1553, 4153, 7, 1225, 5, 13815, 2, *1, *1]\n",
    "\n",
    "        Attention Mask:\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, *0, *0]\n",
    "\n",
    "        Label:\n",
    "        0\n",
    "\n",
    "        Models usually expect inputs to have the same length (max = max_lenght).\n",
    "        If your token list is too short, you add padding tokens (which are just zeros).\n",
    "        \"\"\"\n",
    "        # Fetch the text and its corresponding label using the index\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Tokenize the text:\n",
    "        # - truncation: cuts off texts longer than max_length\n",
    "        # - padding: adds padding tokens to shorter texts\n",
    "        # - return_tensors='pt': returns PyTorch tensors instead of lists\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length = self.max_length,\n",
    "            return_tensors='pt' # stands for PyTorch\n",
    "        )\n",
    "\n",
    "        # Return a dictionary with model inputs and label\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),         # Tensor of input token IDs .squeeze() again just removes the extra dimension.\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(), # Tensor indicating real tokens vs padding\n",
    "            'labels': torch.tensor(label, dtype=torch.long)        # Make sure it becomes a PyTorch Tensor, because the model expects labels to be Tensors during training.\n",
    "        }\n",
    "    \n",
    "\n",
    "from torch.amp import GradScaler, autocast\n",
    "# Use smaller (half-size) numbers for most calculations, so training is much faster and uses less memory ‚Äî without losing much accuracy.\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Mixed Precision Training\n",
    "def train_MPT(model, loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        with autocast(device_type=device.type):  # üî• autocast for mixed precision\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# Evaluation Function\n",
    "def evaluate(model, loader):\n",
    "    model.eval() # Set the model to evaluation mode\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad(): # Turns off gradient calculations (Saves memory and speeds up evaluation because we don't need gradients)\n",
    "        for batch in tqdm(loader):\n",
    "            # Move data to device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            # Get predictions (pick the class with the highest score (probability) for each example in the batch)\n",
    "            preds = torch.argmax(outputs.logits, dim=1) # outputs.logits = raw scores for each class\n",
    "            # Save predictions and true labels\n",
    "            # We Move predictions and labels from GPU to CPU and convert them into numpy arrays\n",
    "            # because some Python operations (like NumPy or scikit-learn) cannot work directly with GPU Tensors.\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Take the F1 score for each label (class) separately, and then average them equally (It does not care if one label has 1000 examples and another class has 100 examples)\n",
    "    macro_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    return macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0912771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Bohua\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3548, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Bohua\\AppData\\Local\\Temp\\ipykernel_2644\\4112795864.py\", line 19, in <module>\n",
      "    tfidf.fit(full_train_texts)\n",
      "  File \"C:\\Users\\Bohua\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Bohua\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py\", line 2074, in fit\n",
      "    X = super().fit_transform(raw_documents)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Bohua\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Bohua\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1376, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Bohua\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py\", line None, in _count_vocab\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Bohua\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 2142, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Bohua\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Bohua\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Bohua\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Bohua\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Bohua\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Bohua\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Bohua\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Bohua\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\stack_data\\core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Bohua\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Bohua\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\stack_data\\core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Bohua\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Bohua\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\stack_data\\core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "           ^^^^^\n",
      "  File \"C:\\Users\\Bohua\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\executing\\executing.py\", line 116, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "# 1. Preprocessing\n",
    "full_train_texts = training_data['sentence'].tolist() + test_data['sentence'].tolist()\n",
    "full_train_labels = training_data['source'].tolist() + test_data['source'].tolist()\n",
    "\n",
    "labels = list(sorted(set(full_train_labels)))\n",
    "label2id = {label: idx for idx, label in enumerate(labels)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "y_train = [label2id[label] for label in training_data['source']]\n",
    "y_dev = [label2id[label] for label in test_data['source']]\n",
    "\n",
    "# 2. TF-IDF Vectorization\n",
    "tfidf = TfidfVectorizer(\n",
    "    analyzer='char_wb',\n",
    "    ngram_range=(2, 6),\n",
    "    max_features=50000\n",
    ")\n",
    "\n",
    "tfidf.fit(full_train_texts)\n",
    "X_train = tfidf.transform(training_data['sentence'])\n",
    "X_dev = tfidf.transform(test_data['sentence'])\n",
    "\n",
    "MODEL_NAME = 'roberta-base'\n",
    "MAX_LENGTH = 128\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 2\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "training_data['label_id'] = training_data['source'].map(label2id)\n",
    "test_data['label_id'] = test_data['source'].map(label2id)\n",
    "\n",
    "train_dataset = RedditDataset(training_data['sentence'].tolist(), training_data['label_id'].tolist(), tokenizer, MAX_LENGTH)\n",
    "dev_dataset = RedditDataset(test_data['sentence'].tolist(), test_data['label_id'].tolist(), tokenizer, MAX_LENGTH)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # cuda is faster but it requires GPU\n",
    "model = RobertaForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(labels))\n",
    "model.to(device) # Moves all model weights and computations to the device you selected (cuda if GPU, otherwise cpu)\n",
    "\n",
    "# Optimizer and Scheduler\n",
    "# model.parameters() are all the learnable weights in the model.\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE) # A type of optimizer that updates the model's weights to minimize the loss. \n",
    "total_steps = len(train_loader) * EPOCHS \n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps) # It controls how the learning rate changes during training. In our case it linearly decreases \n",
    "\n",
    "\n",
    "MODEL_SAVE_PATH = f'cache/best_{MODEL_NAME}_model_{MAX_LENGTH}-{BATCH_SIZE}-{EPOCHS}-{LEARNING_RATE}.pt'\n",
    "\n",
    "# Check if saved model exists\n",
    "if os.path.exists(MODEL_SAVE_PATH):\n",
    "    print(f\"Loading saved model from {MODEL_SAVE_PATH}...\")\n",
    "    model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
    "else:\n",
    "    print(\"No saved model found. Starting training...\")\n",
    "\n",
    "    # Run Training\n",
    "    best_f1 = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(EPOCHS): # One epoch = One full pass through the entire training dataset.\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}\") \n",
    "        train_loss = train_MPT(model, train_loader)\n",
    "        val_f1 = evaluate(model, dev_loader)\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Validation Macro F1: {val_f1:.4f}\")\n",
    "\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            best_model_state = model.state_dict()\n",
    "\n",
    "    print(f'Best f1 for this model: {best_f1}')\n",
    "\n",
    "    # After training, save the best model\n",
    "    torch.save(best_model_state, MODEL_SAVE_PATH)\n",
    "    print(f\"Training finished. Best model saved to {MODEL_SAVE_PATH}\")\n",
    "\n",
    "    # Loads back the best version of the model that you saved during training (Forget your current weights ‚Äî load these saved weights instead)\n",
    "    model.load_state_dict(best_model_state) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59bb8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentence_roberta(text):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_tensors='pt'\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probs = torch.softmax(outputs.logits, dim=1).cpu().numpy().flatten()\n",
    "        pred_idx = np.argmax(probs)\n",
    "        print(f\"\\nüìù Sentence: {text}\")\n",
    "        print(f\"üì¢ Predicted Source: {id2label[pred_idx]}\")\n",
    "        print(\"\\nüîç Top Probabilities:\")\n",
    "        for idx in probs.argsort()[::-1]:\n",
    "            print(f\"  {id2label[idx]:<15} : {probs[idx]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
